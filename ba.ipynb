{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZpFE004pn0zy"
   },
   "outputs": [],
   "source": [
    "# !pip install mxnet-mkl==1.6.0 numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qa9n4VWXoTgr",
    "outputId": "b5b87e93-916e-41a3-ebc1-6a273aae04d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting elephas\n",
      "  Downloading elephas-3.4.8-py3-none-any.whl (34 kB)\n",
      "Collecting h5py==3.3.0\n",
      "  Downloading h5py-3.3.0.tar.gz (380 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.2/380.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[854 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Collecting pkgconfig\n",
      "  \u001b[31m   \u001b[0m   Downloading pkgconfig-1.5.5-py3-none-any.whl (6.7 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting numpy==1.19.3\n",
      "  \u001b[31m   \u001b[0m   Downloading numpy-1.19.3.zip (7.3 MB)\n",
      "  \u001b[31m   \u001b[0m      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.3/7.3 MB 5.2 MB/s eta 0:00:00\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: started\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m Collecting Cython>=0.29.15\n",
      "  \u001b[31m   \u001b[0m   Using cached Cython-0.29.33-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "  \u001b[31m   \u001b[0m Building wheels for collected packages: numpy\n",
      "  \u001b[31m   \u001b[0m   Building wheel for numpy (pyproject.toml): started\n",
      "  \u001b[31m   \u001b[0m   Building wheel for numpy (pyproject.toml): still running...\n",
      "  \u001b[31m   \u001b[0m   Building wheel for numpy (pyproject.toml): still running...\n",
      "  \u001b[31m   \u001b[0m   Building wheel for numpy (pyproject.toml): finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   error: subprocess-exited-with-error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   × Building wheel for numpy (pyproject.toml) did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   │ exit code: 1\n",
      "  \u001b[31m   \u001b[0m   ╰─> [822 lines of output]\n",
      "  \u001b[31m   \u001b[0m       setup.py:67: RuntimeWarning: NumPy 1.19.3 may not yet support Python 3.10.\n",
      "  \u001b[31m   \u001b[0m         warnings.warn(\n",
      "  \u001b[31m   \u001b[0m       Running from numpy source directory.\n",
      "  \u001b[31m   \u001b[0m       Cythonizing sources\n",
      "  \u001b[31m   \u001b[0m       numpy/random/_bounded_integers.pxd.in has not changed\n",
      "  \u001b[31m   \u001b[0m       numpy/random/_sfc64.pyx has not changed\n",
      "  \u001b[31m   \u001b[0m       numpy/random/_common.pyx has not changed\n",
      "  \u001b[31m   \u001b[0m       numpy/random/_pcg64.pyx has not changed\n",
      "  \u001b[31m   \u001b[0m       numpy/random/bit_generator.pyx has not changed\n",
      "  \u001b[31m   \u001b[0m       numpy/random/_mt19937.pyx has not changed\n",
      "  \u001b[31m   \u001b[0m       numpy/random/_philox.pyx has not changed\n",
      "  \u001b[31m   \u001b[0m       Processing numpy/random/_bounded_integers.pyx\n",
      "  \u001b[31m   \u001b[0m       /tmp/pip-install-hm1vzsgg/numpy_36e0c465ea4e4ddf865898e21e1368cc/tools/cythonize.py:67: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  \u001b[31m   \u001b[0m         from distutils.version import LooseVersion\n",
      "  \u001b[31m   \u001b[0m       numpy/random/_generator.pyx has not changed\n",
      "  \u001b[31m   \u001b[0m       numpy/random/mtrand.pyx has not changed\n",
      "  \u001b[31m   \u001b[0m       numpy/random/_bounded_integers.pyx.in has not changed\n",
      "  \u001b[31m   \u001b[0m       blas_opt_info:\n",
      "  \u001b[31m   \u001b[0m       blas_mkl_info:\n",
      "  \u001b[31m   \u001b[0m       customize UnixCCompiler\n",
      "  \u001b[31m   \u001b[0m         libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "  \u001b[31m   \u001b[0m         NOT AVAILABLE\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       blis_info:\n",
      "  \u001b[31m   \u001b[0m         libraries blis not found in ['/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "  \u001b[31m   \u001b[0m         NOT AVAILABLE\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       openblas_info:\n",
      "  \u001b[31m   \u001b[0m       C compiler: x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       creating /tmp/tmp13opg5yg/tmp\n",
      "  \u001b[31m   \u001b[0m       creating /tmp/tmp13opg5yg/tmp/tmp13opg5yg\n",
      "  \u001b[31m   \u001b[0m       compile options: '-c'\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: /tmp/tmp13opg5yg/source.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc /tmp/tmp13opg5yg/tmp/tmp13opg5yg/source.o -lopenblas -o /tmp/tmp13opg5yg/a.out\n",
      "  \u001b[31m   \u001b[0m         FOUND:\n",
      "  \u001b[31m   \u001b[0m           libraries = ['openblas', 'openblas']\n",
      "  \u001b[31m   \u001b[0m           library_dirs = ['/usr/lib/x86_64-linux-gnu']\n",
      "  \u001b[31m   \u001b[0m           language = c\n",
      "  \u001b[31m   \u001b[0m           define_macros = [('HAVE_CBLAS', None)]\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         FOUND:\n",
      "  \u001b[31m   \u001b[0m           libraries = ['openblas', 'openblas']\n",
      "  \u001b[31m   \u001b[0m           library_dirs = ['/usr/lib/x86_64-linux-gnu']\n",
      "  \u001b[31m   \u001b[0m           language = c\n",
      "  \u001b[31m   \u001b[0m           define_macros = [('HAVE_CBLAS', None)]\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       non-existing path in 'numpy/distutils': 'site.cfg'\n",
      "  \u001b[31m   \u001b[0m       lapack_opt_info:\n",
      "  \u001b[31m   \u001b[0m       lapack_mkl_info:\n",
      "  \u001b[31m   \u001b[0m         libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']\n",
      "  \u001b[31m   \u001b[0m         NOT AVAILABLE\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       openblas_lapack_info:\n",
      "  \u001b[31m   \u001b[0m       C compiler: x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       creating /tmp/tmpim4ffebu/tmp\n",
      "  \u001b[31m   \u001b[0m       creating /tmp/tmpim4ffebu/tmp/tmpim4ffebu\n",
      "  \u001b[31m   \u001b[0m       compile options: '-c'\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: /tmp/tmpim4ffebu/source.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc /tmp/tmpim4ffebu/tmp/tmpim4ffebu/source.o -lopenblas -o /tmp/tmpim4ffebu/a.out\n",
      "  \u001b[31m   \u001b[0m         FOUND:\n",
      "  \u001b[31m   \u001b[0m           libraries = ['openblas', 'openblas']\n",
      "  \u001b[31m   \u001b[0m           library_dirs = ['/usr/lib/x86_64-linux-gnu']\n",
      "  \u001b[31m   \u001b[0m           language = c\n",
      "  \u001b[31m   \u001b[0m           define_macros = [('HAVE_CBLAS', None)]\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         FOUND:\n",
      "  \u001b[31m   \u001b[0m           libraries = ['openblas', 'openblas']\n",
      "  \u001b[31m   \u001b[0m           library_dirs = ['/usr/lib/x86_64-linux-gnu']\n",
      "  \u001b[31m   \u001b[0m           language = c\n",
      "  \u001b[31m   \u001b[0m           define_macros = [('HAVE_CBLAS', None)]\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       /usr/lib/python3.10/distutils/dist.py:274: UserWarning: Unknown distribution option: 'define_macros'\n",
      "  \u001b[31m   \u001b[0m         warnings.warn(msg)\n",
      "  \u001b[31m   \u001b[0m       running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m       running build\n",
      "  \u001b[31m   \u001b[0m       running config_cc\n",
      "  \u001b[31m   \u001b[0m       unifing config_cc, config, build_clib, build_ext, build commands --compiler options\n",
      "  \u001b[31m   \u001b[0m       running config_fc\n",
      "  \u001b[31m   \u001b[0m       unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\n",
      "  \u001b[31m   \u001b[0m       running build_src\n",
      "  \u001b[31m   \u001b[0m       build_src\n",
      "  \u001b[31m   \u001b[0m       building py_modules sources\n",
      "  \u001b[31m   \u001b[0m       building library \"npymath\" sources\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable gfortran\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable f95\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable ifort\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable ifc\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable lf95\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable pgfortran\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable nvfortran\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable f90\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable f77\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable fort\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable efort\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable efc\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable g77\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable g95\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable pathf95\n",
      "  \u001b[31m   \u001b[0m       Could not locate executable nagfor\n",
      "  \u001b[31m   \u001b[0m       don't know how to compile Fortran code on platform 'posix'\n",
      "  \u001b[31m   \u001b[0m         adding 'build/src.linux-x86_64-3.10/numpy/core/src/npymath' to include_dirs.\n",
      "  \u001b[31m   \u001b[0m       None - nothing done with h_files = ['build/src.linux-x86_64-3.10/numpy/core/src/npymath/npy_math_internal.h']\n",
      "  \u001b[31m   \u001b[0m       building library \"npysort\" sources\n",
      "  \u001b[31m   \u001b[0m         adding 'build/src.linux-x86_64-3.10/numpy/core/src/common' to include_dirs.\n",
      "  \u001b[31m   \u001b[0m       None - nothing done with h_files = ['build/src.linux-x86_64-3.10/numpy/core/src/common/npy_sort.h', 'build/src.linux-x86_64-3.10/numpy/core/src/common/npy_partition.h', 'build/src.linux-x86_64-3.10/numpy/core/src/common/npy_binsearch.h']\n",
      "  \u001b[31m   \u001b[0m       building library \"npyrandom\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.core._multiarray_tests\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.core._multiarray_umath\" sources\n",
      "  \u001b[31m   \u001b[0m         adding 'build/src.linux-x86_64-3.10/numpy/core/src/umath' to include_dirs.\n",
      "  \u001b[31m   \u001b[0m         adding 'build/src.linux-x86_64-3.10/numpy/core/src/npymath' to include_dirs.\n",
      "  \u001b[31m   \u001b[0m         adding 'build/src.linux-x86_64-3.10/numpy/core/src/common' to include_dirs.\n",
      "  \u001b[31m   \u001b[0m       numpy.core - nothing done with h_files = ['build/src.linux-x86_64-3.10/numpy/core/src/umath/funcs.inc', 'build/src.linux-x86_64-3.10/numpy/core/src/umath/simd.inc', 'build/src.linux-x86_64-3.10/numpy/core/src/umath/loops.h', 'build/src.linux-x86_64-3.10/numpy/core/src/umath/matmul.h', 'build/src.linux-x86_64-3.10/numpy/core/src/umath/clip.h', 'build/src.linux-x86_64-3.10/numpy/core/src/npymath/npy_math_internal.h', 'build/src.linux-x86_64-3.10/numpy/core/src/common/templ_common.h', 'build/src.linux-x86_64-3.10/numpy/core/include/numpy/config.h', 'build/src.linux-x86_64-3.10/numpy/core/include/numpy/_numpyconfig.h', 'build/src.linux-x86_64-3.10/numpy/core/include/numpy/__multiarray_api.h', 'build/src.linux-x86_64-3.10/numpy/core/include/numpy/__ufunc_api.h']\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.core._umath_tests\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.core._rational_tests\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.core._struct_ufunc_tests\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.core._operand_flag_tests\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.fft._pocketfft_internal\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.linalg.lapack_lite\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.linalg._umath_linalg\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.random._mt19937\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.random._philox\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.random._pcg64\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.random._sfc64\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.random._common\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.random.bit_generator\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.random._generator\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.random._bounded_integers\" sources\n",
      "  \u001b[31m   \u001b[0m       building extension \"numpy.random.mtrand\" sources\n",
      "  \u001b[31m   \u001b[0m       building data_files sources\n",
      "  \u001b[31m   \u001b[0m       build_src: building npy-pkg config files\n",
      "  \u001b[31m   \u001b[0m       running build_py\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying numpy/_pytesttester.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying numpy/dual.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying numpy/version.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ctypeslib.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matlib.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying numpy/_globals.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying numpy/conftest.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying numpy/__init__.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying numpy/setup.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying numpy/_distributor_init.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       copying build/src.linux-x86_64-3.10/numpy/__config__.py -> build/lib.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/compat\n",
      "  \u001b[31m   \u001b[0m       copying numpy/compat/__init__.py -> build/lib.linux-x86_64-3.10/numpy/compat\n",
      "  \u001b[31m   \u001b[0m       copying numpy/compat/setup.py -> build/lib.linux-x86_64-3.10/numpy/compat\n",
      "  \u001b[31m   \u001b[0m       copying numpy/compat/py3k.py -> build/lib.linux-x86_64-3.10/numpy/compat\n",
      "  \u001b[31m   \u001b[0m       copying numpy/compat/_inspect.py -> build/lib.linux-x86_64-3.10/numpy/compat\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/compat/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/compat/tests/test_compat.py -> build/lib.linux-x86_64-3.10/numpy/compat/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/compat/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/compat/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/numerictypes.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/_type_aliases.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/setup_common.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/records.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/numeric.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/machar.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/fromnumeric.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/arrayprint.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/_exceptions.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/umath_tests.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/getlimits.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/umath.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/overrides.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/_asarray.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/_internal.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/__init__.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/setup.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/_ufunc_config.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/_dtype_ctypes.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/_add_newdocs.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/function_base.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/_string_helpers.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/defchararray.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/memmap.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/multiarray.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/_dtype.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/cversions.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/_methods.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/einsumfunc.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/shape_base.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/code_generators/generate_numpy_api.py -> build/lib.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_extint128.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_api.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_item_selection.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_overrides.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/_locales.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_shape_base.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_half.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_indexerrors.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_function_base.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_nditer.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_scalarmath.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_scalarinherit.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_unicode.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_print.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_regression.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_longdouble.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test__exceptions.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_memmap.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_indexing.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_umath_complex.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_records.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_defchararray.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_conversion_utils.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_getlimits.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_cpu_features.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_multiarray.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_datetime.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_arrayprint.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_scalar_methods.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_deprecations.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_einsum.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_machar.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_abc.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_protocols.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_scalarprint.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_dtype.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_numeric.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_numerictypes.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_scalar_ctors.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_ufunc.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_umath.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_mem_overlap.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_scalarbuffer.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_errstate.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/core/tests/test_umath_accuracy.py -> build/lib.linux-x86_64-3.10/numpy/core/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/cpuinfo.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/mingw32ccompiler.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/msvccompiler.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/system_info.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/pathccompiler.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/conv_template.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/exec_command.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/extension.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/from_template.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/core.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/__init__.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/setup.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/_shell_utils.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/lib2def.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/unixccompiler.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/misc_util.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/log.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/numpy_distribution.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/line_endings.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/msvc9compiler.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/npy_pkg_config.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/ccompiler.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/intelccompiler.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       copying build/src.linux-x86_64-3.10/numpy/distutils/__config__.py -> build/lib.linux-x86_64-3.10/numpy/distutils\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/egg_info.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/build_src.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/sdist.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/build_scripts.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/build_py.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/build_clib.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/install_headers.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/config_compiler.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/install_clib.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/install.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/develop.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/install_data.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/bdist_rpm.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/__init__.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/build.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/autodist.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/build_ext.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/command/config.py -> build/lib.linux-x86_64-3.10/numpy/distutils/command\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/lahey.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/g95.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/none.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/intel.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/nag.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/pg.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/environment.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/nv.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/vast.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/__init__.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/gnu.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/absoft.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/ibm.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/compaq.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/sun.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/hpux.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/pathf95.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/fcompiler/mips.py -> build/lib.linux-x86_64-3.10/numpy/distutils/fcompiler\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_npy_pkg_config.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_shell_utils.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_exec_command.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_mingw32ccompiler.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_fcompiler_nagfor.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_misc_util.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_fcompiler_intel.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_system_info.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_from_template.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_fcompiler_gnu.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/distutils/tests/test_fcompiler.py -> build/lib.linux-x86_64-3.10/numpy/distutils/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/ufuncs.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/indexing.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/broadcasting.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/basics.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/__init__.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/internals.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/dispatch.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/byteswapping.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/constants.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/misc.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/structured_arrays.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/subclassing.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/glossary.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       copying numpy/doc/creation.py -> build/lib.linux-x86_64-3.10/numpy/doc\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/f2py_testing.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/__main__.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/__version__.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/cb_rules.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/common_rules.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/auxfuncs.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/rules.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/capi_maps.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/diagnose.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/f90mod_rules.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/__init__.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/setup.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/f2py2e.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/use_rules.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/cfuncs.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/func2subr.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/crackfortran.py -> build/lib.linux-x86_64-3.10/numpy/f2py\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_return_character.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_compile_function.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_assumed_shape.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_block_docstring.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/util.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_regression.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_quoted_character.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_return_integer.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_common.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_semicolon_split.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_size.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_crackfortran.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_parameter.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_mixed.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_string.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_return_logical.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_kind.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_return_complex.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_callback.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_array_from_pyobj.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/f2py/tests/test_return_real.py -> build/lib.linux-x86_64-3.10/numpy/f2py/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/fft\n",
      "  \u001b[31m   \u001b[0m       copying numpy/fft/helper.py -> build/lib.linux-x86_64-3.10/numpy/fft\n",
      "  \u001b[31m   \u001b[0m       copying numpy/fft/_pocketfft.py -> build/lib.linux-x86_64-3.10/numpy/fft\n",
      "  \u001b[31m   \u001b[0m       copying numpy/fft/__init__.py -> build/lib.linux-x86_64-3.10/numpy/fft\n",
      "  \u001b[31m   \u001b[0m       copying numpy/fft/setup.py -> build/lib.linux-x86_64-3.10/numpy/fft\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/fft/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/fft/tests/test_helper.py -> build/lib.linux-x86_64-3.10/numpy/fft/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/fft/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/fft/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/fft/tests/test_pocketfft.py -> build/lib.linux-x86_64-3.10/numpy/fft/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/arraypad.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/ufunclike.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/polynomial.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/mixins.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/user_array.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/type_check.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/nanfunctions.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/format.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/arrayterator.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/utils.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/twodim_base.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/financial.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/__init__.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/histograms.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/index_tricks.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/setup.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/arraysetops.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/function_base.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/_iotools.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/npyio.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/_datasource.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/scimath.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/stride_tricks.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/_version.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/recfunctions.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/shape_base.py -> build/lib.linux-x86_64-3.10/numpy/lib\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_arraysetops.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_histograms.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_type_check.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_utils.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_shape_base.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_function_base.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_regression.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_recfunctions.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test__version.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_packbits.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_nanfunctions.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_stride_tricks.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_index_tricks.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_polynomial.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_financial.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_io.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_arraypad.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test__datasource.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_ufunclike.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test__iotools.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_arrayterator.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_twodim_base.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_format.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/lib/tests/test_mixins.py -> build/lib.linux-x86_64-3.10/numpy/lib/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/linalg\n",
      "  \u001b[31m   \u001b[0m       copying numpy/linalg/linalg.py -> build/lib.linux-x86_64-3.10/numpy/linalg\n",
      "  \u001b[31m   \u001b[0m       copying numpy/linalg/__init__.py -> build/lib.linux-x86_64-3.10/numpy/linalg\n",
      "  \u001b[31m   \u001b[0m       copying numpy/linalg/setup.py -> build/lib.linux-x86_64-3.10/numpy/linalg\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/linalg/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/linalg/tests/test_regression.py -> build/lib.linux-x86_64-3.10/numpy/linalg/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/linalg/tests/test_linalg.py -> build/lib.linux-x86_64-3.10/numpy/linalg/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/linalg/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/linalg/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/linalg/tests/test_deprecations.py -> build/lib.linux-x86_64-3.10/numpy/linalg/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/linalg/tests/test_build.py -> build/lib.linux-x86_64-3.10/numpy/linalg/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/ma\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/extras.py -> build/lib.linux-x86_64-3.10/numpy/ma\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/testutils.py -> build/lib.linux-x86_64-3.10/numpy/ma\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/bench.py -> build/lib.linux-x86_64-3.10/numpy/ma\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/mrecords.py -> build/lib.linux-x86_64-3.10/numpy/ma\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/core.py -> build/lib.linux-x86_64-3.10/numpy/ma\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/__init__.py -> build/lib.linux-x86_64-3.10/numpy/ma\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/setup.py -> build/lib.linux-x86_64-3.10/numpy/ma\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/timer_comparison.py -> build/lib.linux-x86_64-3.10/numpy/ma\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/ma/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/tests/test_mrecords.py -> build/lib.linux-x86_64-3.10/numpy/ma/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/tests/test_regression.py -> build/lib.linux-x86_64-3.10/numpy/ma/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/tests/test_extras.py -> build/lib.linux-x86_64-3.10/numpy/ma/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/tests/test_old_ma.py -> build/lib.linux-x86_64-3.10/numpy/ma/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/ma/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/tests/test_deprecations.py -> build/lib.linux-x86_64-3.10/numpy/ma/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/tests/test_subclassing.py -> build/lib.linux-x86_64-3.10/numpy/ma/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/ma/tests/test_core.py -> build/lib.linux-x86_64-3.10/numpy/ma/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/matrixlib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/__init__.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/setup.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/defmatrix.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/matrixlib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/tests/test_masked_matrix.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/tests/test_regression.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/tests/test_defmatrix.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/tests/test_multiarray.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/tests/test_interaction.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/tests/test_numeric.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/matrixlib/tests/test_matrix_linalg.py -> build/lib.linux-x86_64-3.10/numpy/matrixlib/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/polynomial.py -> build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/laguerre.py -> build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/_polybase.py -> build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/hermite_e.py -> build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/legendre.py -> build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/__init__.py -> build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/setup.py -> build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/hermite.py -> build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/polyutils.py -> build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/chebyshev.py -> build/lib.linux-x86_64-3.10/numpy/polynomial\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/tests/test_classes.py -> build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/tests/test_printing.py -> build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/tests/test_hermite_e.py -> build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/tests/test_polyutils.py -> build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/tests/test_polynomial.py -> build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/tests/test_laguerre.py -> build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/tests/test_legendre.py -> build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/tests/test_hermite.py -> build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/polynomial/tests/test_chebyshev.py -> build/lib.linux-x86_64-3.10/numpy/polynomial/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/random\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/_pickle.py -> build/lib.linux-x86_64-3.10/numpy/random\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/__init__.py -> build/lib.linux-x86_64-3.10/numpy/random\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/setup.py -> build/lib.linux-x86_64-3.10/numpy/random\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/test_randomstate_regression.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/test_smoke.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/test_random.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/test_regression.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/test_generator_mt19937_regressions.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/test_extending.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/test_direct.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/test_seed_sequence.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/test_generator_mt19937.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/random/tests/test_randomstate.py -> build/lib.linux-x86_64-3.10/numpy/random/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/testing\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/utils.py -> build/lib.linux-x86_64-3.10/numpy/testing\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/__init__.py -> build/lib.linux-x86_64-3.10/numpy/testing\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/setup.py -> build/lib.linux-x86_64-3.10/numpy/testing\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/print_coercion_tables.py -> build/lib.linux-x86_64-3.10/numpy/testing\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/testing/_private\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/_private/noseclasses.py -> build/lib.linux-x86_64-3.10/numpy/testing/_private\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/_private/utils.py -> build/lib.linux-x86_64-3.10/numpy/testing/_private\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/_private/decorators.py -> build/lib.linux-x86_64-3.10/numpy/testing/_private\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/_private/__init__.py -> build/lib.linux-x86_64-3.10/numpy/testing/_private\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/_private/parameterized.py -> build/lib.linux-x86_64-3.10/numpy/testing/_private\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/_private/nosetester.py -> build/lib.linux-x86_64-3.10/numpy/testing/_private\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/testing/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/tests/test_utils.py -> build/lib.linux-x86_64-3.10/numpy/testing/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/testing/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/tests/test_doctesting.py -> build/lib.linux-x86_64-3.10/numpy/testing/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/testing/tests/test_decorators.py -> build/lib.linux-x86_64-3.10/numpy/testing/tests\n",
      "  \u001b[31m   \u001b[0m       creating build/lib.linux-x86_64-3.10/numpy/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/tests/test_scripts.py -> build/lib.linux-x86_64-3.10/numpy/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/tests/test_warnings.py -> build/lib.linux-x86_64-3.10/numpy/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/tests/test_numpy_version.py -> build/lib.linux-x86_64-3.10/numpy/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/tests/__init__.py -> build/lib.linux-x86_64-3.10/numpy/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/tests/test_matlib.py -> build/lib.linux-x86_64-3.10/numpy/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/tests/test_public_api.py -> build/lib.linux-x86_64-3.10/numpy/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/tests/test_ctypeslib.py -> build/lib.linux-x86_64-3.10/numpy/tests\n",
      "  \u001b[31m   \u001b[0m       copying numpy/tests/test_reloading.py -> build/lib.linux-x86_64-3.10/numpy/tests\n",
      "  \u001b[31m   \u001b[0m       running build_clib\n",
      "  \u001b[31m   \u001b[0m       customize UnixCCompiler\n",
      "  \u001b[31m   \u001b[0m       customize UnixCCompiler using new_build_clib\n",
      "  \u001b[31m   \u001b[0m       building 'npymath' library\n",
      "  \u001b[31m   \u001b[0m       compiling C sources\n",
      "  \u001b[31m   \u001b[0m       C compiler: x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/numpy/core/src\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/numpy/core/src/npymath\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/build\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy/core\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy/core/src\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy/core/src/npymath\n",
      "  \u001b[31m   \u001b[0m       compile options: '-Ibuild/src.linux-x86_64-3.10/numpy/core/src/npymath -Inumpy/core/include -Ibuild/src.linux-x86_64-3.10/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.10 -Ibuild/src.linux-x86_64-3.10/numpy/core/src/common -Ibuild/src.linux-x86_64-3.10/numpy/core/src/npymath -c'\n",
      "  \u001b[31m   \u001b[0m       extra options: '-std=c99'\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/npymath/npy_math.cx86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npymath/ieee754.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npymath/npy_math_complex.c\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/npymath/halffloat.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc-ar: adding 4 object files to build/temp.linux-x86_64-3.10/libnpymath.a\n",
      "  \u001b[31m   \u001b[0m       building 'npysort' library\n",
      "  \u001b[31m   \u001b[0m       compiling C sources\n",
      "  \u001b[31m   \u001b[0m       C compiler: x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy/core/src/npysort\n",
      "  \u001b[31m   \u001b[0m       compile options: '-Ibuild/src.linux-x86_64-3.10/numpy/core/src/common -Inumpy/core/include -Ibuild/src.linux-x86_64-3.10/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.10 -Ibuild/src.linux-x86_64-3.10/numpy/core/src/common -Ibuild/src.linux-x86_64-3.10/numpy/core/src/npymath -c'\n",
      "  \u001b[31m   \u001b[0m       extra options: '-std=c99'\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npysort/mergesort.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npysort/quicksort.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npysort/timsort.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npysort/heapsort.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npysort/radixsort.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npysort/selection.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npysort/binsearch.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc-ar: adding 7 object files to build/temp.linux-x86_64-3.10/libnpysort.a\n",
      "  \u001b[31m   \u001b[0m       building 'npyrandom' library\n",
      "  \u001b[31m   \u001b[0m       compiling C sources\n",
      "  \u001b[31m   \u001b[0m       C compiler: x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/numpy/random\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/numpy/random/src\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/numpy/random/src/distributions\n",
      "  \u001b[31m   \u001b[0m       compile options: '-Inumpy/core/include -Ibuild/src.linux-x86_64-3.10/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.10 -Ibuild/src.linux-x86_64-3.10/numpy/core/src/common -Ibuild/src.linux-x86_64-3.10/numpy/core/src/npymath -c'\n",
      "  \u001b[31m   \u001b[0m       extra options: '-std=c99'\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/random/src/distributions/logfactorial.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/random/src/distributions/distributions.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/random/src/distributions/random_mvhg_count.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/random/src/distributions/random_mvhg_marginals.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/random/src/distributions/random_hypergeometric.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc-ar: adding 5 object files to build/temp.linux-x86_64-3.10/libnpyrandom.a\n",
      "  \u001b[31m   \u001b[0m       running build_ext\n",
      "  \u001b[31m   \u001b[0m       customize UnixCCompiler\n",
      "  \u001b[31m   \u001b[0m       customize UnixCCompiler using new_build_ext\n",
      "  \u001b[31m   \u001b[0m       building 'numpy.core._multiarray_tests' extension\n",
      "  \u001b[31m   \u001b[0m       compiling C sources\n",
      "  \u001b[31m   \u001b[0m       C compiler: x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy/core/src/multiarray\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/numpy/core/src/common\n",
      "  \u001b[31m   \u001b[0m       compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.linux-x86_64-3.10/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.10 -Ibuild/src.linux-x86_64-3.10/numpy/core/src/common -Ibuild/src.linux-x86_64-3.10/numpy/core/src/npymath -c'\n",
      "  \u001b[31m   \u001b[0m       extra options: '-std=c99'\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/multiarray/_multiarray_tests.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/common/mem_overlap.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy/core/src/multiarray/_multiarray_tests.o build/temp.linux-x86_64-3.10/numpy/core/src/common/mem_overlap.o -Lbuild/temp.linux-x86_64-3.10 -lnpymath -o build/lib.linux-x86_64-3.10/numpy/core/_multiarray_tests.cpython-310-x86_64-linux-gnu.so\n",
      "  \u001b[31m   \u001b[0m       building 'numpy.core._multiarray_umath' extension\n",
      "  \u001b[31m   \u001b[0m       compiling C sources\n",
      "  \u001b[31m   \u001b[0m       C compiler: x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/numpy/core/src/multiarray\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/numpy/core/src/umath\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy/core/src/umath\n",
      "  \u001b[31m   \u001b[0m       creating build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy/core/src/common\n",
      "  \u001b[31m   \u001b[0m       compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DHAVE_CBLAS -Ibuild/src.linux-x86_64-3.10/numpy/core/src/umath -Ibuild/src.linux-x86_64-3.10/numpy/core/src/npymath -Ibuild/src.linux-x86_64-3.10/numpy/core/src/common -Inumpy/core/include -Ibuild/src.linux-x86_64-3.10/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.10 -Ibuild/src.linux-x86_64-3.10/numpy/core/src/common -Ibuild/src.linux-x86_64-3.10/numpy/core/src/npymath -c'\n",
      "  \u001b[31m   \u001b[0m       extra options: '-std=c99'\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/alloc.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/arrayfunction_override.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/datetime_strings.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/convert.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/arrayobject.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/buffer.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/convert_datatype.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/datetime_busday.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/calculation.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/conversion_utils.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/datetime_busdaycal.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/ctors.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/multiarray/arraytypes.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/compiled_base.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/descriptor.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/common.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/datetime.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/dtype_transfer.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/dragon4.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/item_selection.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/multiarraymodule.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/multiarray/einsum.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/iterators.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/multiarray/lowlevel_strided_loops.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/multiarray/nditer_templ.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/array_assign_scalar.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/array_assign_array.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/nditer_api.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/flagsobject.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/number.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/getset.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/hashdescr.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/nditer_constr.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/refcount.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/multiarray/scalartypes.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/sequence.c\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src: In function ‘float_arrtype_hash’:\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2967:27: error: incompatible type for argument 1 of ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2967 |     return _Py_HashDouble((double) PyArrayScalar_VAL(obj, @name@));\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:38: note: expected ‘PyObject *’ {aka ‘struct _object *’} but argument is of type ‘double’\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                                      ^~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2967:12: error: too few arguments to function ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2967 |     return _Py_HashDouble((double) PyArrayScalar_VAL(obj, @name@));\n",
      "  \u001b[31m   \u001b[0m             |            ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:23: note: declared here\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                       ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src: In function ‘cfloat_arrtype_hash’:\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2975:31: error: incompatible type for argument 1 of ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2975 |     hashreal = _Py_HashDouble((double)\n",
      "  \u001b[31m   \u001b[0m             |                               ^~~~~~~~\n",
      "  \u001b[31m   \u001b[0m             |                               |\n",
      "  \u001b[31m   \u001b[0m             |                               double\n",
      "  \u001b[31m   \u001b[0m        2976 |             PyArrayScalar_VAL(obj, C@name@).real);\n",
      "  \u001b[31m   \u001b[0m             |             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:38: note: expected ‘PyObject *’ {aka ‘struct _object *’} but argument is of type ‘double’\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                                      ^~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2975:16: error: too few arguments to function ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2975 |     hashreal = _Py_HashDouble((double)\n",
      "  \u001b[31m   \u001b[0m             |                ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:23: note: declared here\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                       ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2981:31: error: incompatible type for argument 1 of ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2981 |     hashimag = _Py_HashDouble((double)\n",
      "  \u001b[31m   \u001b[0m             |                               ^~~~~~~~\n",
      "  \u001b[31m   \u001b[0m             |                               |\n",
      "  \u001b[31m   \u001b[0m             |                               double\n",
      "  \u001b[31m   \u001b[0m        2982 |             PyArrayScalar_VAL(obj, C@name@).imag);\n",
      "  \u001b[31m   \u001b[0m             |             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:38: note: expected ‘PyObject *’ {aka ‘struct _object *’} but argument is of type ‘double’\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                                      ^~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2981:16: error: too few arguments to function ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2981 |     hashimag = _Py_HashDouble((double)\n",
      "  \u001b[31m   \u001b[0m             |                ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:23: note: declared here\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                       ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src: In function ‘longdouble_arrtype_hash’:\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2967:27: error: incompatible type for argument 1 of ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2967 |     return _Py_HashDouble((double) PyArrayScalar_VAL(obj, @name@));\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:38: note: expected ‘PyObject *’ {aka ‘struct _object *’} but argument is of type ‘double’\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                                      ^~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2967:12: error: too few arguments to function ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2967 |     return _Py_HashDouble((double) PyArrayScalar_VAL(obj, @name@));\n",
      "  \u001b[31m   \u001b[0m             |            ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:23: note: declared here\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                       ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src: In function ‘clongdouble_arrtype_hash’:\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2975:31: error: incompatible type for argument 1 of ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2975 |     hashreal = _Py_HashDouble((double)\n",
      "  \u001b[31m   \u001b[0m             |                               ^~~~~~~~\n",
      "  \u001b[31m   \u001b[0m             |                               |\n",
      "  \u001b[31m   \u001b[0m             |                               double\n",
      "  \u001b[31m   \u001b[0m        2976 |             PyArrayScalar_VAL(obj, C@name@).real);\n",
      "  \u001b[31m   \u001b[0m             |             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:38: note: expected ‘PyObject *’ {aka ‘struct _object *’} but argument is of type ‘double’\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                                      ^~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2975:16: error: too few arguments to function ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2975 |     hashreal = _Py_HashDouble((double)\n",
      "  \u001b[31m   \u001b[0m             |                ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:23: note: declared here\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                       ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2981:31: error: incompatible type for argument 1 of ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2981 |     hashimag = _Py_HashDouble((double)\n",
      "  \u001b[31m   \u001b[0m             |                               ^~~~~~~~\n",
      "  \u001b[31m   \u001b[0m             |                               |\n",
      "  \u001b[31m   \u001b[0m             |                               double\n",
      "  \u001b[31m   \u001b[0m        2982 |             PyArrayScalar_VAL(obj, C@name@).imag);\n",
      "  \u001b[31m   \u001b[0m             |             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:38: note: expected ‘PyObject *’ {aka ‘struct _object *’} but argument is of type ‘double’\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                                      ^~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2981:16: error: too few arguments to function ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2981 |     hashimag = _Py_HashDouble((double)\n",
      "  \u001b[31m   \u001b[0m             |                ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:23: note: declared here\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                       ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src: In function ‘half_arrtype_hash’:\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2997:27: error: incompatible type for argument 1 of ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2997 |     return _Py_HashDouble(npy_half_to_double(PyArrayScalar_VAL(obj, Half)));\n",
      "  \u001b[31m   \u001b[0m             |                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m             |                           |\n",
      "  \u001b[31m   \u001b[0m             |                           double\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:38: note: expected ‘PyObject *’ {aka ‘struct _object *’} but argument is of type ‘double’\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                                      ^~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2997:12: error: too few arguments to function ‘_Py_HashDouble’\n",
      "  \u001b[31m   \u001b[0m        2997 |     return _Py_HashDouble(npy_half_to_double(PyArrayScalar_VAL(obj, Half)));\n",
      "  \u001b[31m   \u001b[0m             |            ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       In file included from /usr/include/python3.10/Python.h:77,\n",
      "  \u001b[31m   \u001b[0m                        from numpy/core/src/multiarray/scalartypes.c.src:3:\n",
      "  \u001b[31m   \u001b[0m       /usr/include/python3.10/pyhash.h:10:23: note: declared here\n",
      "  \u001b[31m   \u001b[0m          10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);\n",
      "  \u001b[31m   \u001b[0m             |                       ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src: In function ‘longdouble_arrtype_hash’:\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2968:1: warning: control reaches end of non-void function [-Wreturn-type]\n",
      "  \u001b[31m   \u001b[0m        2968 | }\n",
      "  \u001b[31m   \u001b[0m             | ^\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src: In function ‘float_arrtype_hash’:\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2968:1: warning: control reaches end of non-void function [-Wreturn-type]\n",
      "  \u001b[31m   \u001b[0m        2968 | }\n",
      "  \u001b[31m   \u001b[0m             | ^\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src: In function ‘half_arrtype_hash’:\n",
      "  \u001b[31m   \u001b[0m       numpy/core/src/multiarray/scalartypes.c.src:2998:1: warning: control reaches end of non-void function [-Wreturn-type]\n",
      "  \u001b[31m   \u001b[0m        2998 | }\n",
      "  \u001b[31m   \u001b[0m             | ^\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/vdot.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/shape.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/umath/umathmodule.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/umath/reduction.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/scalarapi.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/umath/loops.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/umath/clip.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/nditer_pywrap.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/umath/ufunc_object.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/umath/override.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/npymath/npy_math.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npymath/ieee754.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/npymath/npy_math_complex.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/npymath/halffloat.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/common/array_assign.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/common/mem_overlap.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/umath/extobj.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/umath/scalarmath.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/common/npy_longdouble.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/common/ucsnarrow.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/common/ufunc_override.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/common/numpyos.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/common/npy_cpu_features.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/common/cblasfuncs.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/common/python_xerbla.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/umath/ufunc_type_resolution.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/mapping.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: numpy/core/src/multiarray/methods.c\n",
      "  \u001b[31m   \u001b[0m       x86_64-linux-gnu-gcc: build/src.linux-x86_64-3.10/numpy/core/src/umath/matmul.c\n",
      "  \u001b[31m   \u001b[0m       error: Command \"x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DHAVE_CBLAS -Ibuild/src.linux-x86_64-3.10/numpy/core/src/umath -Ibuild/src.linux-x86_64-3.10/numpy/core/src/npymath -Ibuild/src.linux-x86_64-3.10/numpy/core/src/common -Inumpy/core/include -Ibuild/src.linux-x86_64-3.10/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.10 -Ibuild/src.linux-x86_64-3.10/numpy/core/src/common -Ibuild/src.linux-x86_64-3.10/numpy/core/src/npymath -c build/src.linux-x86_64-3.10/numpy/core/src/multiarray/scalartypes.c -o build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy/core/src/multiarray/scalartypes.o -MMD -MF build/temp.linux-x86_64-3.10/build/src.linux-x86_64-3.10/numpy/core/src/multiarray/scalartypes.o.d -std=c99\" failed with exit status 1\n",
      "  \u001b[31m   \u001b[0m       [end of output]\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m   ERROR: Failed building wheel for numpy\n",
      "  \u001b[31m   \u001b[0m Failed to build numpy\n",
      "  \u001b[31m   \u001b[0m ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m [notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "  \u001b[31m   \u001b[0m [notice] To update, run: python3 -m pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install elephas\n",
    "# \n",
    "# exit() # restart the runtime to refresh installed pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Box_Ihr3oDDW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xXiqb_dkn0z5"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fE3jK2M5J07n"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJPAjdP3J0vE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2rRSDgsomPx"
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"data/ba-walmart.zip\") as zf:\n",
    "#     zf.extractall(\n",
    "#         \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KnqYrJ4Kn0z7"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E_DNqmTyn0z8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 05:17:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Vu dep trai\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EC-LvMMgIn77"
   },
   "outputs": [],
   "source": [
    "sc= spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "us1moTa6TEHC"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/ba-walmart/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/ba-walmart/test.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/ba-walmart/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/ba-walmart/features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/ba-walmart/test.csv'"
     ]
    }
   ],
   "source": [
    "# test = pd.read_csv('data/ba-walmart/test.csv')\n",
    "# train = pd.read_csv('data/ba-walmart/train.csv')\n",
    "# features = pd.read_csv('data/ba-walmart/features.csv')\n",
    "# stores = pd.read_csv('data/ba-walmart/stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "snZ7yoy1Ti8W",
    "outputId": "a12e60a9-d5b4-4a34-d43d-f883f46731c1"
   },
   "outputs": [],
   "source": [
    "# features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nt6UeV9Rn0z-",
    "outputId": "b6986245-00cb-4237-fab4-7e54012bdb24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|Store|Type|  Size|\n",
      "+-----+----+------+\n",
      "|    1|   A|151315|\n",
      "|    2|   A|202307|\n",
      "|    3|   B| 37392|\n",
      "|    4|   A|205863|\n",
      "|    5|   B| 34875|\n",
      "+-----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stores_raw = spark.read.csv(\"data/ba-walmart/stores.csv\", header=True, inferSchema=True)\n",
    "df_stores_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Pk7jX0zn0z_",
    "outputId": "fa27aa50-77e6-4b83-87db-d79e416c9bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "|Store|               Date|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|        CPI|Unemployment|IsHoliday|\n",
      "+-----+-------------------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "|    1|2010-02-05 00:00:00|      42.31|     2.572|       NA|       NA|       NA|       NA|       NA|211.0963582|       8.106|    false|\n",
      "|    1|2010-02-12 00:00:00|      38.51|     2.548|       NA|       NA|       NA|       NA|       NA|211.2421698|       8.106|     true|\n",
      "|    1|2010-02-19 00:00:00|      39.93|     2.514|       NA|       NA|       NA|       NA|       NA|211.2891429|       8.106|    false|\n",
      "|    1|2010-02-26 00:00:00|      46.63|     2.561|       NA|       NA|       NA|       NA|       NA|211.3196429|       8.106|    false|\n",
      "|    1|2010-03-05 00:00:00|       46.5|     2.625|       NA|       NA|       NA|       NA|       NA|211.3501429|       8.106|    false|\n",
      "+-----+-------------------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_feature_raw = spark.read.csv(\"data/ba-walmart/features.csv\", header=True, inferSchema=True)\n",
    "df_feature_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZASv1J-n00A",
    "outputId": "77b04702-4f96-481c-f267-5ea7eb551ab6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------------+------------+---------+\n",
      "|Store|Dept|               Date|Weekly_Sales|IsHoliday|\n",
      "+-----+----+-------------------+------------+---------+\n",
      "|    1|   1|2010-02-05 00:00:00|     24924.5|    false|\n",
      "|    1|   1|2010-02-12 00:00:00|    46039.49|     true|\n",
      "|    1|   1|2010-02-19 00:00:00|    41595.55|    false|\n",
      "|    1|   1|2010-02-26 00:00:00|    19403.54|    false|\n",
      "|    1|   1|2010-03-05 00:00:00|     21827.9|    false|\n",
      "+-----+----+-------------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_raw = spark.read.csv(\"data/ba-walmart/train.csv\", header=True, inferSchema=True)\n",
    "df_test_raw = spark.read.csv(\"data/ba-walmart/test.csv\", header=True, inferSchema=True)\n",
    "df_train_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCL2kHWDn00B",
    "outputId": "301d6843-53b6-4ede-f6d4-9866c29d3a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------+----------+-----------+------------+---------+\n",
      "|Store|               Date|Temperature|Fuel_Price|        CPI|Unemployment|IsHoliday|\n",
      "+-----+-------------------+-----------+----------+-----------+------------+---------+\n",
      "|    1|2010-02-05 00:00:00|      42.31|     2.572|211.0963582|       8.106|    false|\n",
      "|    1|2010-02-12 00:00:00|      38.51|     2.548|211.2421698|       8.106|     true|\n",
      "|    1|2010-02-19 00:00:00|      39.93|     2.514|211.2891429|       8.106|    false|\n",
      "|    1|2010-02-26 00:00:00|      46.63|     2.561|211.3196429|       8.106|    false|\n",
      "|    1|2010-03-05 00:00:00|       46.5|     2.625|211.3501429|       8.106|    false|\n",
      "+-----+-------------------+-----------+----------+-----------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_feature_raw drop Markdown\n",
    "df_feature = df_feature_raw.drop(\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\")\n",
    "df_feature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgb1kTnwn00C",
    "outputId": "4a086a4f-e96f-4553-cd92-7a08e1a8b5aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- Dept: integer (nullable = true)\n",
      " |-- Weekly_Sales: double (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- CPI: float (nullable = true)\n",
      " |-- Unemployment: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      "\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+\n",
      "|Store|               Date|IsHoliday|Dept|Weekly_Sales|Temperature|Fuel_Price|      CPI|Unemployment|Type|  Size|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+\n",
      "|    1|2010-02-05 00:00:00|    false|   1|     24924.5|      42.31|     2.572|211.09636|       8.106|   A|151315|\n",
      "|    1|2010-02-12 00:00:00|     true|   1|    46039.49|      38.51|     2.548|211.24217|       8.106|   A|151315|\n",
      "|    1|2010-02-19 00:00:00|    false|   1|    41595.55|      39.93|     2.514|211.28914|       8.106|   A|151315|\n",
      "|    1|2010-02-26 00:00:00|    false|   1|    19403.54|      46.63|     2.561|211.31964|       8.106|   A|151315|\n",
      "|    1|2010-03-05 00:00:00|    false|   1|     21827.9|       46.5|     2.625|211.35014|       8.106|   A|151315|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge df_train and df_feature, df_store, join left\n",
    "df_train = df_train_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"], ).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "df_test = df_test_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"]).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "# convert CPI, Unemployment to float\n",
    "df_train = df_train.withColumn(\"CPI\", df_train[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df_train[\"Unemployment\"].cast(DoubleType()))\n",
    "df_test = df_test.withColumn(\"CPI\", df_test[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df_test[\"Unemployment\"].cast(DoubleType()))\n",
    "# summary info df_train\n",
    "df_train.printSchema()\n",
    "df_train.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdaZbMsfn00E",
    "outputId": "d7b6e8df-c0e8-486e-cba7-37dfb0641022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "|Store|               Date|IsHoliday|Dept|Weekly_Sales|Temperature|Fuel_Price|      CPI|Unemployment|Type|  Size|Year|Month|Week|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "|    1|2010-02-05 00:00:00|        0|   1|     24924.5|      42.31|     2.572|211.09636|       8.106|   A|151315|2010|    2|   5|\n",
      "|    1|2010-02-12 00:00:00|        1|   1|    46039.49|      38.51|     2.548|211.24217|       8.106|   A|151315|2010|    2|   6|\n",
      "|    1|2010-02-19 00:00:00|        0|   1|    41595.55|      39.93|     2.514|211.28914|       8.106|   A|151315|2010|    2|   7|\n",
      "|    1|2010-02-26 00:00:00|        0|   1|    19403.54|      46.63|     2.561|211.31964|       8.106|   A|151315|2010|    2|   8|\n",
      "|    1|2010-03-05 00:00:00|        0|   1|     21827.9|       46.5|     2.625|211.35014|       8.106|   A|151315|2010|    3|   9|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df_test = df_test.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df_train = df_train.withColumn(\"IsHoliday\", df_train[\"IsHoliday\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"IsHoliday\", df_test[\"IsHoliday\"].cast(IntegerType()))\n",
    "df_train.createOrReplaceTempView(\"df_train\")\n",
    "df_test.createOrReplaceTempView(\"df_test\")\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CGxh857En00F"
   },
   "outputs": [],
   "source": [
    "# # draw pie chart of store type given by df_stores\n",
    "# df_stores = df_stores_raw.toPandas()\n",
    "# df_stores[\"Type\"].value_counts().plot.pie(autopct=\"%1.1f%%\", figsize=(6, 6), labels=None).set_title(\"Store Type\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.legend(labels=df_stores[\"Type\"].value_counts().index, loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BStsy-59n00G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/bitnami/spark/python/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "df_train_pd = df_train.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCyhxLf6Sl_S",
    "outputId": "7c0a369c-17d2-46f3-eb0b-c7aa10bbc76f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store           0\n",
       "Date            0\n",
       "IsHoliday       0\n",
       "Dept            0\n",
       "Weekly_Sales    0\n",
       "Temperature     0\n",
       "Fuel_Price      0\n",
       "CPI             0\n",
       "Unemployment    0\n",
       "Type            0\n",
       "Size            0\n",
       "Year            0\n",
       "Month           0\n",
       "Week            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "g66Iqs47n00G"
   },
   "outputs": [],
   "source": [
    "#sns.scatterplot(x=\"Size\", y=\"Weekly_Sales\", data=df_train_pd, hue=\"Type\", alpha=0.2).set_title(\"Size, Type vs Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EzVMzfTtn00H"
   },
   "outputs": [],
   "source": [
    "# df_train_pd.groupby(\"IsHoliday\")[\"Weekly_Sales\"].mean().plot.bar().set_title(\"IsHoliday vs Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "j7boUuZ3n00I"
   },
   "outputs": [],
   "source": [
    "# sns.scatterplot(x=\"Fuel_Price\", y=\"Weekly_Sales\", hue=\"Type\", data=df_train_pd, alpha=0.2).set_title(\"Fuel_Price vs Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pP-1RQein00I"
   },
   "outputs": [],
   "source": [
    "# find relationship between Unemployment and Weekly_Sales\n",
    "# sns.scatterplot(x=\"Unemployment\", y=\"Weekly_Sales\", hue=\"Type\", data=df_train_pd, alpha=0.2).set_title(\"Unemployment vs Weekly_Sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Kvn7Hqbmn00J"
   },
   "outputs": [],
   "source": [
    "# find relationship between CPI and Weekly_Sales\n",
    "# sns.scatterplot(x=\"CPI\", y=\"Weekly_Sales\", hue=\"Type\", data=df_train_pd, alpha=0.2).set_title(\"CPI vs Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "o291Bva8n00J"
   },
   "outputs": [],
   "source": [
    "# find relationship between Temperature and Weekly_Sales\n",
    "# sns.scatterplot(x=\"Temperature\", y=\"Weekly_Sales\", hue=\"Type\", data=df_train_pd, alpha=0.2).set_title(\"Temperature vs Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uBLbMcUTn00K"
   },
   "outputs": [],
   "source": [
    "# average_sales_by_time = spark.sql(\"SELECT Year, Month, AVG(Weekly_Sales) AS Average_Sales FROM df_train GROUP BY Year, Month ORDER BY Year, Month\")\n",
    "# average_sales_by_time_pd = average_sales_by_time.toPandas()\n",
    "# sns.barplot(x=\"Month\", y=\"Average_Sales\", hue='Year', data=average_sales_by_time_pd).set_title(\"Average Sales by Month and Year\")\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKpbmyc-n00L",
    "outputId": "f3fb87b1-20c1-4b05-b3d7-137cf8afb398"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/bitnami/spark/python/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "|Store|               Date|IsHoliday|Dept|Weekly_Sales|Temperature|Fuel_Price|      CPI|Unemployment|Type|  Size|Year|Month|Week|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "|    1|2010-02-05 00:00:00|        0|   1|     24924.5|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|\n",
      "|    1|2010-02-12 00:00:00|        1|   1|    46039.49|      38.51|     2.548|211.24217|       8.106|   0|151315|2010|    2|   6|\n",
      "|    1|2010-02-19 00:00:00|        0|   1|    41595.55|      39.93|     2.514|211.28914|       8.106|   0|151315|2010|    2|   7|\n",
      "|    1|2010-02-26 00:00:00|        0|   1|    19403.54|      46.63|     2.561|211.31964|       8.106|   0|151315|2010|    2|   8|\n",
      "|    1|2010-03-05 00:00:00|        0|   1|     21827.9|       46.5|     2.625|211.35014|       8.106|   0|151315|2010|    3|   9|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "types = df_train.select(\"Type\").distinct().collect()\n",
    "types.sort()\n",
    "mapping = {t.Type: str(i) for i, t in enumerate(types)}\n",
    "df_train = df_train.replace(mapping, subset=[\"Type\"])\n",
    "df_test = df_test.replace(mapping, subset=[\"Type\"])\n",
    "df_train = df_train.withColumn(\"Type\", df_train[\"Type\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"Type\", df_test[\"Type\"].cast(IntegerType()))\n",
    "df_train_pd = df_train.toPandas()\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtWRt3gvn00L",
    "outputId": "928342ab-a9be-4cb7-a498-d673ef7201ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly_Sales    1.000000\n",
      "Size            0.243828\n",
      "Dept            0.148032\n",
      "Month           0.028409\n",
      "Week            0.027673\n",
      "IsHoliday       0.012774\n",
      "Fuel_Price     -0.000120\n",
      "Temperature    -0.002312\n",
      "Year           -0.010111\n",
      "CPI            -0.020921\n",
      "Unemployment   -0.025864\n",
      "Store          -0.085195\n",
      "Type           -0.182242\n",
      "Name: Weekly_Sales, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6000/3010913235.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  results = df_train_pd.corr()[\"Weekly_Sales\"].sort_values(ascending=False)\n"
     ]
    }
   ],
   "source": [
    "# find correlation between attributes and Weekly_Sales\n",
    "results = df_train_pd.corr()[\"Weekly_Sales\"].sort_values(ascending=False)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAvlAQVcn00M",
    "outputId": "6346071d-d12e-4ea4-b573-5b581f76f6e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training attribute:  ['Weekly_Sales' 'Size' 'Dept' 'Month' 'Week' 'IsHoliday' 'Year' 'CPI'\n",
      " 'Unemployment' 'Store' 'Type']\n",
      "Ignore attribute:  {'Temperature', 'Fuel_Price', 'Date'}\n"
     ]
    }
   ],
   "source": [
    "keep_cols = results[results.abs() > 0.01].index.values\n",
    "print(\"Training attribute: \", keep_cols)\n",
    "ignore_attributes = set(df_train_pd.columns) - set(keep_cols)\n",
    "print(\"Ignore attribute: \", ignore_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "n6ijk8PVn00M"
   },
   "outputs": [],
   "source": [
    "df_train_clean = df_train.drop(*ignore_attributes)\n",
    "df_test_clean = df_test.drop(*ignore_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TO9SO_Qk4RME",
    "outputId": "9f3b20dc-2609-40b6-ac7e-e353300f1dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----+------------+---------+------------+----+------+----+-----+----+\n",
      "|Store|IsHoliday|Dept|Weekly_Sales|CPI      |Unemployment|Type|Size  |Year|Month|Week|\n",
      "+-----+---------+----+------------+---------+------------+----+------+----+-----+----+\n",
      "|1    |0        |1   |24924.5     |211.09636|8.106       |0   |151315|2010|2    |5   |\n",
      "|1    |1        |1   |46039.49    |211.24217|8.106       |0   |151315|2010|2    |6   |\n",
      "|1    |0        |1   |41595.55    |211.28914|8.106       |0   |151315|2010|2    |7   |\n",
      "|1    |0        |1   |19403.54    |211.31964|8.106       |0   |151315|2010|2    |8   |\n",
      "|1    |0        |1   |21827.9     |211.35014|8.106       |0   |151315|2010|3    |9   |\n",
      "|1    |0        |1   |21043.39    |211.38065|8.106       |0   |151315|2010|3    |10  |\n",
      "|1    |0        |1   |22136.64    |211.21564|8.106       |0   |151315|2010|3    |11  |\n",
      "|1    |0        |1   |26229.21    |211.01804|8.106       |0   |151315|2010|3    |12  |\n",
      "|1    |0        |1   |57258.43    |210.82045|7.808       |0   |151315|2010|4    |13  |\n",
      "|1    |0        |1   |42960.91    |210.62286|7.808       |0   |151315|2010|4    |14  |\n",
      "|1    |0        |1   |17596.96    |210.4887 |7.808       |0   |151315|2010|4    |15  |\n",
      "|1    |0        |1   |16145.35    |210.43912|7.808       |0   |151315|2010|4    |16  |\n",
      "|1    |0        |1   |16555.11    |210.38954|7.808       |0   |151315|2010|4    |17  |\n",
      "|1    |0        |1   |17413.94    |210.33997|7.808       |0   |151315|2010|5    |18  |\n",
      "|1    |0        |1   |18926.74    |210.33743|7.808       |0   |151315|2010|5    |19  |\n",
      "|1    |0        |1   |14773.04    |210.6171 |7.808       |0   |151315|2010|5    |20  |\n",
      "|1    |0        |1   |15580.43    |210.89676|7.808       |0   |151315|2010|5    |21  |\n",
      "|1    |0        |1   |17558.09    |211.17642|7.808       |0   |151315|2010|6    |22  |\n",
      "|1    |0        |1   |16637.62    |211.4561 |7.808       |0   |151315|2010|6    |23  |\n",
      "|1    |0        |1   |16216.27    |211.45377|7.808       |0   |151315|2010|6    |24  |\n",
      "|1    |0        |1   |16328.72    |211.33865|7.808       |0   |151315|2010|6    |25  |\n",
      "|1    |0        |1   |16333.14    |211.22353|7.787       |0   |151315|2010|7    |26  |\n",
      "|1    |0        |1   |17688.76    |211.10841|7.787       |0   |151315|2010|7    |27  |\n",
      "|1    |0        |1   |17150.84    |211.10039|7.787       |0   |151315|2010|7    |28  |\n",
      "|1    |0        |1   |15360.45    |211.23514|7.787       |0   |151315|2010|7    |29  |\n",
      "|1    |0        |1   |15381.82    |211.3699 |7.787       |0   |151315|2010|7    |30  |\n",
      "|1    |0        |1   |17508.41    |211.50467|7.787       |0   |151315|2010|8    |31  |\n",
      "|1    |0        |1   |15536.4     |211.63942|7.787       |0   |151315|2010|8    |32  |\n",
      "|1    |0        |1   |15740.13    |211.60336|7.787       |0   |151315|2010|8    |33  |\n",
      "|1    |0        |1   |15793.87    |211.5673 |7.787       |0   |151315|2010|8    |34  |\n",
      "|1    |0        |1   |16241.78    |211.53125|7.787       |0   |151315|2010|9    |35  |\n",
      "|1    |1        |1   |18194.74    |211.4952 |7.787       |0   |151315|2010|9    |36  |\n",
      "|1    |0        |1   |19354.23    |211.52246|7.787       |0   |151315|2010|9    |37  |\n",
      "|1    |0        |1   |18122.52    |211.59723|7.787       |0   |151315|2010|9    |38  |\n",
      "|1    |0        |1   |20094.19    |211.672  |7.838       |0   |151315|2010|10   |39  |\n",
      "|1    |0        |1   |23388.03    |211.74675|7.838       |0   |151315|2010|10   |40  |\n",
      "|1    |0        |1   |26978.34    |211.81375|7.838       |0   |151315|2010|10   |41  |\n",
      "|1    |0        |1   |25543.04    |211.8613 |7.838       |0   |151315|2010|10   |42  |\n",
      "|1    |0        |1   |38640.93    |211.90884|7.838       |0   |151315|2010|10   |43  |\n",
      "|1    |0        |1   |34238.88    |211.95639|7.838       |0   |151315|2010|11   |44  |\n",
      "|1    |0        |1   |19549.39    |212.00394|7.838       |0   |151315|2010|11   |45  |\n",
      "|1    |0        |1   |19552.84    |211.88968|7.838       |0   |151315|2010|11   |46  |\n",
      "|1    |1        |1   |18820.29    |211.74843|7.838       |0   |151315|2010|11   |47  |\n",
      "|1    |0        |1   |22517.56    |211.6072 |7.838       |0   |151315|2010|12   |48  |\n",
      "|1    |0        |1   |31497.65    |211.46596|7.838       |0   |151315|2010|12   |49  |\n",
      "|1    |0        |1   |44912.86    |211.40532|7.838       |0   |151315|2010|12   |50  |\n",
      "|1    |0        |1   |55931.23    |211.40512|7.838       |0   |151315|2010|12   |51  |\n",
      "|1    |1        |1   |19124.58    |211.40494|7.838       |0   |151315|2010|12   |52  |\n",
      "|1    |0        |1   |15984.24    |211.40474|7.742       |0   |151315|2011|1    |1   |\n",
      "|1    |0        |1   |17359.7     |211.45741|7.742       |0   |151315|2011|1    |2   |\n",
      "+-----+---------+----+------------+---------+------------+----+------+----+-----+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_clean.show(50, truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ptx_3p9x0pc9"
   },
   "outputs": [],
   "source": [
    "# (trainingDF, validationDF) = train_df.withColumn(\"label\", train_df[\"class\"].cast(DoubleType())+lit(1)).select(\"features\", \"label\").randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3vqXsmiZ0eQR"
   },
   "outputs": [],
   "source": [
    "# train_df.withColumn(\"label\", train_df[\"class\"].cast(DoubleType())+lit(1)).select(\"features\", \"label\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "8Ini6zfGzrdN"
   },
   "outputs": [],
   "source": [
    "# trainingDF.show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "fQCwS_4vAPc1"
   },
   "outputs": [],
   "source": [
    "df_train =df_train.drop(\"Date\")\n",
    "df_train =df_train.drop(\"Unemployment\")\n",
    "df_train =df_train.drop(\"CPI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----+------------+-----------+----------+----+------+----+-----+----+\n",
      "|Store|IsHoliday|Dept|Weekly_Sales|Temperature|Fuel_Price|Type|  Size|Year|Month|Week|\n",
      "+-----+---------+----+------------+-----------+----------+----+------+----+-----+----+\n",
      "|    1|        0|   1|     24924.5|      42.31|     2.572|   0|151315|2010|    2|   5|\n",
      "|    1|        1|   1|    46039.49|      38.51|     2.548|   0|151315|2010|    2|   6|\n",
      "|    1|        0|   1|    41595.55|      39.93|     2.514|   0|151315|2010|    2|   7|\n",
      "|    1|        0|   1|    19403.54|      46.63|     2.561|   0|151315|2010|    2|   8|\n",
      "|    1|        0|   1|     21827.9|       46.5|     2.625|   0|151315|2010|    3|   9|\n",
      "|    1|        0|   1|    21043.39|      57.79|     2.667|   0|151315|2010|    3|  10|\n",
      "|    1|        0|   1|    22136.64|      54.58|      2.72|   0|151315|2010|    3|  11|\n",
      "|    1|        0|   1|    26229.21|      51.45|     2.732|   0|151315|2010|    3|  12|\n",
      "|    1|        0|   1|    57258.43|      62.27|     2.719|   0|151315|2010|    4|  13|\n",
      "|    1|        0|   1|    42960.91|      65.86|      2.77|   0|151315|2010|    4|  14|\n",
      "|    1|        0|   1|    17596.96|      66.32|     2.808|   0|151315|2010|    4|  15|\n",
      "|    1|        0|   1|    16145.35|      64.84|     2.795|   0|151315|2010|    4|  16|\n",
      "|    1|        0|   1|    16555.11|      67.41|      2.78|   0|151315|2010|    4|  17|\n",
      "|    1|        0|   1|    17413.94|      72.55|     2.835|   0|151315|2010|    5|  18|\n",
      "|    1|        0|   1|    18926.74|      74.78|     2.854|   0|151315|2010|    5|  19|\n",
      "|    1|        0|   1|    14773.04|      76.44|     2.826|   0|151315|2010|    5|  20|\n",
      "|    1|        0|   1|    15580.43|      80.44|     2.759|   0|151315|2010|    5|  21|\n",
      "|    1|        0|   1|    17558.09|      80.69|     2.705|   0|151315|2010|    6|  22|\n",
      "|    1|        0|   1|    16637.62|      80.43|     2.668|   0|151315|2010|    6|  23|\n",
      "|    1|        0|   1|    16216.27|      84.11|     2.637|   0|151315|2010|    6|  24|\n",
      "+-----+---------+----+------------+-----------+----------+----+------+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "TCkWR9v5n00N"
   },
   "outputs": [],
   "source": [
    "# create regression model\n",
    "input_cols = df_train.columns\n",
    "# input_cols = ['CPI', 'Unemployment', 'IsHoliday', 'Week','Weekly_Sales']\n",
    "input_cols.remove(\"Weekly_Sales\")\n",
    "output_col = \"Weekly_Sales\"\n",
    "assembler = VectorAssembler( outputCol=\"preFeatures\")\n",
    "assembler.setInputCols(input_cols)\n",
    "assembler.setHandleInvalid(\"skip\")\n",
    "#df_train-_clean_ = df_train_clean_.withColumn(\"weight\", when(df_train_clean_[\"IsHoliday\"] == 1, 5).otherwise(1))\n",
    "assemble_train_df = assembler.transform(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store           0\n",
       "IsHoliday       0\n",
       "Dept            0\n",
       "Weekly_Sales    0\n",
       "Temperature     0\n",
       "Fuel_Price      0\n",
       "Type            0\n",
       "Size            0\n",
       "Year            0\n",
       "Month           0\n",
       "Week            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check df_val has nan\n",
    "df_train_pd = df_train.toPandas()\n",
    "df_train_pd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ZIoHjpLC0TGq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df_train_clean_, df_val) = assemble_train_df.select(\"preFeatures\", \"Weekly_Sales\").randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "saler = MinMaxScaler(inputCol=\"preFeatures\", outputCol=\"features\")\n",
    "saler = saler.fit(df_train_clean_)\n",
    "scaledData = saler.transform(df_train_clean_)\n",
    "scaledData_valid = saler.transform(df_val)\n",
    "# scaler = MinMaxScaler(inputCol=\"preFeatures\", outputCol=\"features\")\n",
    "# # rescale each feature to range [min, max].\n",
    "# scaledData = scaler.fit(df_train_clean_).transform(df_train_clean_).select(\"features\", \"Weekly_Sales\")\n",
    "# scaledData_valid = scaler.fit(df_train_clean_).transform(df_val).select(\"features\", \"Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+\n",
      "|         preFeatures|Weekly_Sales|            features|\n",
      "+--------------------+------------+--------------------+\n",
      "|[1.0,0.0,1.0,35.4...|     17359.7|(10,[3,4,6,7,9],[...|\n",
      "|[1.0,0.0,1.0,39.9...|    41595.55|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,42.3...|     24924.5|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,43.8...|    18461.18|(10,[3,4,6,7,9],[...|\n",
      "|[1.0,0.0,1.0,43.9...|    33305.92|[0.0,0.0,0.0,0.45...|\n",
      "|[1.0,0.0,1.0,45.3...|     54060.1|[0.0,0.0,0.0,0.46...|\n",
      "|[1.0,0.0,1.0,46.5...|     21827.9|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,46.6...|    19403.54|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,47.9...|    46788.75|[0.0,0.0,0.0,0.48...|\n",
      "|[1.0,0.0,1.0,48.2...|    15984.24|(10,[3,4,6,7],[0....|\n",
      "|[1.0,0.0,1.0,48.9...|    25293.49|[0.0,0.0,0.0,0.49...|\n",
      "|[1.0,0.0,1.0,49.0...|    16567.69|(10,[3,4,6,7],[0....|\n",
      "|[1.0,0.0,1.0,49.2...|    22517.56|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,49.8...|    44912.86|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,51.4...|    19552.84|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,51.6...|    45773.03|[0.0,0.0,0.0,0.52...|\n",
      "|[1.0,0.0,1.0,52.3...|    55931.23|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,53.5...|     21280.4|[0.0,0.0,0.0,0.54...|\n",
      "|[1.0,0.0,1.0,54.2...|    18378.16|(10,[3,4,6,7,9],[...|\n",
      "|[1.0,0.0,1.0,54.5...|    22136.64|(10,[3,4,6,8,9],[...|\n",
      "+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "25FXdeCwLJFQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 05:18:01.313139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:01.456124: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:01.456155: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-04 05:18:01.475625: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-04 05:18:02.060430: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:02.060568: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:02.060578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-04 05:18:03.685148: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:03.685166: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 05:18:03.685179: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9878222aad48): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 05:18:03.685340: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "# from tensorflow.keras.utils import to_categorical, generic_utils\n",
    "\n",
    "# nb_classes = train_df.select(\"category\").distinct().count()\n",
    "input_dim = len(scaledData.select(\"features\").first()[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_shape=(input_dim,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "zgJeOWB7LNI6",
    "outputId": "25dff2b8-22a7-48bc-fd3c-cf4982ff8abe"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Initialize SparkML Estimator and set all relevant properties\n",
    "# estimator = ElephasEstimator()\n",
    "# estimator.setFeaturesCol(\"scaled_features\")             # These two come directly from pyspark,\n",
    "# estimator.setLabelCol(\"index_category\")                 # hence the camel case. Sorry :)\n",
    "# estimator.set_keras_model_config(model.to_yaml())       # Provide serialized Keras model\n",
    "# estimator.set_categorical_labels(True)\n",
    "# estimator.set_nb_classes(nb_classes)\n",
    "# estimator.set_num_workers(1)  # We just use one worker here. Feel free to adapt it.\n",
    "# estimator.set_epochs(20) \n",
    "# estimator.set_batch_size(128)\n",
    "# estimator.set_verbosity(1)\n",
    "# estimator.set_validation_split(0.15)\n",
    "# estimator.set_optimizer_config(opt_conf)\n",
    "# estimator.set_mode(\"synchronous\")\n",
    "# estimator.set_loss(\"categorical_crossentropy\")\n",
    "# estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_U_r5J5Ohgn",
    "outputId": "f3ddd32b-66fc-45b0-8f66-6c72d9f02ea3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/bitnami/python/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_72bdf11503e8"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elephas.ml_model import ElephasEstimator\n",
    "from tensorflow.keras import optimizers\n",
    "# from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n",
    "# Initialize Spark ML Estimator\n",
    "estimator = ElephasEstimator()\n",
    "estimator.set_keras_model_config(model.to_json())\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.setLabelCol(\"Weekly_Sales\") \n",
    "estimator.set_mode(\"synchronous\")\n",
    "estimator.set_loss(\"mae\")\n",
    "estimator.set_metrics(['mse'])\n",
    "estimator.set_epochs(5)\n",
    "estimator.set_batch_size(32)\n",
    "estimator.set_validation_split(0.1)\n",
    "estimator.set_categorical_labels(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gy64Xw5-LU9x",
    "outputId": "7b0d2775-5f94-48d8-933a-6639b2f84bce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 05:18:11.053089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:11.063117: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:11.064439: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:11.068429: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:11.072995: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:11.075199: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:11.088121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:11.094894: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:11.231584: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:11.231618: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-04 05:18:11.240025: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:11.240182: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-04 05:18:11.240388: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:11.240442: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-04 05:18:11.250808: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:11.250894: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-04 05:18:11.256703: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:11.256776: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-04 05:18:11.262577: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:11.262641: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-04 05:18:11.270578: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-04 05:18:11.282087: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-04 05:18:11.284095: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:11.284162: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-04 05:18:11.285195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:11.285277: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-04 05:18:11.286205: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-04 05:18:11.286474: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-04 05:18:11.297012: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-04 05:18:11.308269: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-04 05:18:11.322075: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-04 05:18:11.328867: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-04 05:18:11.975865: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:11.975940: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:11.975948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-04 05:18:12.003230: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.003287: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.003295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-04 05:18:12.241162: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.241246: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.241260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-04 05:18:12.246941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.247100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.247114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-04 05:18:12.261794: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.261873: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.261883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-04 05:18:12.287003: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.287077: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.287087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-04 05:18:12.325705: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.325912: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.325955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-04 05:18:12.340007: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.340115: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.340129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-04 05:18:12.964761: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:12.964784: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 05:18:12.964801: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9878222aad48): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 05:18:12.964978: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:13.278387: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:13.278412: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 05:18:13.278433: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9878222aad48): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 05:18:13.278683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:13.332989: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:13.333013: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 05:18:13.333031: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9878222aad48): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 05:18:13.333243: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:13.627551: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:13.627595: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 05:18:13.627628: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9878222aad48): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 05:18:13.627906: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:13.710011: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:13.710011: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:13.710050: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 05:18:13.710049: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 05:18:13.710087: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9878222aad48): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 05:18:13.710087: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9878222aad48): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 05:18:13.710414: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:13.710413: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:14.299361: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:14.299385: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 05:18:14.299404: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9878222aad48): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 05:18:14.299635: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:18:14.308637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-02-04 05:18:14.308667: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 05:18:14.308690: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9878222aad48): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 05:18:14.308946: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[Stage 59:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Synchronous training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fitted_pipeline = estimator.fit(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+\n",
      "|         preFeatures|Weekly_Sales|            features|\n",
      "+--------------------+------------+--------------------+\n",
      "|[1.0,0.0,1.0,42.2...|    21665.76|[0.0,0.0,0.0,0.43...|\n",
      "|[1.0,0.0,1.0,44.0...|    17341.47|(10,[3,4,6,7,9],[...|\n",
      "|[1.0,0.0,1.0,46.3...|    31497.65|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,48.5...|     16894.4|(10,[3,4,6,7,9],[...|\n",
      "|[1.0,0.0,1.0,51.4...|    26229.21|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,54.1...|     18365.1|(10,[3,4,6,7,9],[...|\n",
      "|[1.0,0.0,1.0,57.3...|    46845.87|[0.0,0.0,0.0,0.58...|\n",
      "|[1.0,0.0,1.0,59.5...|    20327.61|[0.0,0.0,0.0,0.60...|\n",
      "|[1.0,0.0,1.0,64.6...|    20138.19|[0.0,0.0,0.0,0.65...|\n",
      "|[1.0,0.0,1.0,64.7...|    22366.88|[0.0,0.0,0.0,0.65...|\n",
      "|[1.0,0.0,1.0,64.8...|    16145.35|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,65.9...|     22107.7|[0.0,0.0,0.0,0.66...|\n",
      "|[1.0,0.0,1.0,66.5...|     31579.9|[0.0,0.0,0.0,0.67...|\n",
      "|[1.0,0.0,1.0,67.4...|    16555.11|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,69.1...|    27390.81|[0.0,0.0,0.0,0.69...|\n",
      "|[1.0,0.0,1.0,70.4...|    57592.12|[0.0,0.0,0.0,0.70...|\n",
      "|[1.0,0.0,1.0,74.7...|    18926.74|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,80.9...|    16333.14|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,84.1...|    16216.27|(10,[3,4,6,8,9],[...|\n",
      "|[1.0,0.0,1.0,85.0...|    16119.92|[0.0,0.0,0.0,0.85...|\n",
      "+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check scaledData_valid has nan\n",
    "scaledData_valid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# A fatal error has been detected by the Java Runtime Environment:\n",
      "#\n",
      "#  SIGSEGV (0xb) at pc=0x00007f1348e5a2f8, pid=6030, tid=7515\n",
      "#\n",
      "# JRE version: OpenJDK Runtime Environment (11.0.16+8) (build 11.0.16+8-post-Debian-1deb11u1)\n",
      "# Java VM: OpenJDK 64-Bit Server VM (11.0.16+8-post-Debian-1deb11u1, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64)\n",
      "# Problematic frame:\n",
      "# J 17431 c2 org.apache.spark.sql.catalyst.expressions.UnsafeRow.getArray(I)Lorg/apache/spark/sql/catalyst/util/ArrayData; (6 bytes) @ 0x00007f1348e5a2f8 [0x00007f1348e5a1c0+0x0000000000000138]\n",
      "#\n",
      "# Core dump will be written. Default location: Core dumps may be processed with \"/usr/share/apport/apport -p%p -s%s -c%c -d%d -P%P -u%u -g%g -- %E\" (or dumping to /home/gianglt2/project/BA/spark/core.6030)\n",
      "#\n",
      "# An error report file with more information is saved as:\n",
      "# /home/gianglt2/project/BA/spark/hs_err_pid6030.log\n",
      "[thread 7527 also had an error]\n",
      "Compiled method (c1)   76713 20596       3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificOrdering::compare_0_1$ (95 bytes)\n",
      " total in heap  [0x00007f134181f210,0x00007f134181fc58] = 2632\n",
      " relocation     [0x00007f134181f388,0x00007f134181f420] = 152\n",
      " main code      [0x00007f134181f420,0x00007f134181fa40] = 1568\n",
      " stub code      [0x00007f134181fa40,0x00007f134181fac0] = 128\n",
      " oops           [0x00007f134181fac0,0x00007f134181fac8] = 8\n",
      " metadata       [0x00007f134181fac8,0x00007f134181fad8] = 16\n",
      " scopes data    [0x00007f134181fad8,0x00007f134181fb80] = 168\n",
      " scopes pcs     [0x00007f134181fb80,0x00007f134181fc30] = 176\n",
      " dependencies   [0x00007f134181fc30,0x00007f134181fc38] = 8\n",
      " nul chk table  [0x00007f134181fc38,0x00007f134181fc58] = 32\n",
      "Compiled method (c1)   76727 20605       3       org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificOrdering::compare_0_0$ (319 bytes)\n",
      " total in heap  [0x00007f134146a910,0x00007f134146c258] = 6472\n",
      " relocation     [0x00007f134146aa88,0x00007f134146abe0] = 344\n",
      " main code      [0x00007f134146abe0,0x00007f134146bdc0] = 4576\n",
      " stub code      [0x00007f134146bdc0,0x00007f134146bec0] = 256\n",
      " oops           [0x00007f134146bec0,0x00007f134146bec8] = 8\n",
      " metadata       [0x00007f134146bec8,0x00007f134146bed8] = 16\n",
      " scopes data    [0x00007f134146bed8,0x00007f134146c100] = 552\n",
      " scopes pcs     [0x00007f134146c100,0x00007f134146c230] = 304\n",
      " dependencies   [0x00007f134146c230,0x00007f134146c238] = 8\n",
      " nul chk table  [0x00007f134146c238,0x00007f134146c258] = 32\n",
      "Could not load hsdis-amd64.so; library not loadable; PrintAssembly is disabled\n",
      "[thread 7530 also had an error][thread 7518 also had an error][thread 7544 also had an error]\n",
      "\n",
      "\n",
      "[thread 7531 also had an error]\n",
      "[thread 7516 also had an error]\n",
      "#\n",
      "# If you would like to submit a bug report, please visit:\n",
      "#   https://bugs.debian.org/openjdk-11\n",
      "#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 40422)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/bitnami/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/bitnami/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/opt/bitnami/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/bitnami/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Evaluate Spark model by evaluating the underlying model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m score \u001b[39m=\u001b[39m fitted_pipeline\u001b[39m.\u001b[39;49mtransform(scaledData_valid)\u001b[39m.\u001b[39;49mselect(\u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mWeekly_Sales\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mrdd\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m x: (x[\u001b[39m0\u001b[39;49m], x[\u001b[39m1\u001b[39;49m]))\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m      3\u001b[0m \u001b[39m# prediction = fitted_pipeline.transform(scaledData)\u001b[39;00m\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD\u001b[39m.\u001b[39mcollectAndServe(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd\u001b[39m.\u001b[39mrdd())\n\u001b[1;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/traceback_utils.py:81\u001b[0m, in \u001b[0;36mSCCallSiteSync.__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     79\u001b[0m SCCallSiteSync\u001b[39m.\u001b[39m_spark_stack_depth \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m SCCallSiteSync\u001b[39m.\u001b[39m_spark_stack_depth \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msetCallSite(\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Evaluate Spark model by evaluating the underlying model\n",
    "score = fitted_pipeline.transform(scaledData_valid).select(\"prediction\", \"Weekly_Sales\").rdd.map(lambda x: (x[0], x[1])).collect()\n",
    "# prediction = fitted_pipeline.transform(scaledData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 04:55:51 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:943)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:942)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3838/0x000000084149f840: (struct<Store_double_VectorAssembler_ee238b6831b1:double,IsHoliday_double_VectorAssembler_ee238b6831b1:double,Dept_double_VectorAssembler_ee238b6831b1:double,Temperature:double,Fuel_Price:double,CPI_double_VectorAssembler_ee238b6831b1:double,Unemployment:double,Type_double_VectorAssembler_ee238b6831b1:double,Size_double_VectorAssembler_ee238b6831b1:double,Year_double_VectorAssembler_ee238b6831b1:double,Month_double_VectorAssembler_ee238b6831b1:double,Week_double_VectorAssembler_ee238b6831b1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\t... 5 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 15 more\n",
      "23/02/04 04:55:51 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3838/0x000000084149f840: (struct<Store_double_VectorAssembler_ee238b6831b1:double,IsHoliday_double_VectorAssembler_ee238b6831b1:double,Dept_double_VectorAssembler_ee238b6831b1:double,Temperature:double,Fuel_Price:double,CPI_double_VectorAssembler_ee238b6831b1:double,Unemployment:double,Type_double_VectorAssembler_ee238b6831b1:double,Size_double_VectorAssembler_ee238b6831b1:double,Year_double_VectorAssembler_ee238b6831b1:double,Month_double_VectorAssembler_ee238b6831b1:double,Week_double_VectorAssembler_ee238b6831b1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 15 more\n",
      "23/02/04 04:55:51 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3838/0x000000084149f840: (struct<Store_double_VectorAssembler_ee238b6831b1:double,IsHoliday_double_VectorAssembler_ee238b6831b1:double,Dept_double_VectorAssembler_ee238b6831b1:double,Temperature:double,Fuel_Price:double,CPI_double_VectorAssembler_ee238b6831b1:double,Unemployment:double,Type_double_VectorAssembler_ee238b6831b1:double,Size_double_VectorAssembler_ee238b6831b1:double,Year_double_VectorAssembler_ee238b6831b1:double,Month_double_VectorAssembler_ee238b6831b1:double,Week_double_VectorAssembler_ee238b6831b1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 15 more\n",
      "23/02/04 04:55:51 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3838/0x000000084149f840: (struct<Store_double_VectorAssembler_ee238b6831b1:double,IsHoliday_double_VectorAssembler_ee238b6831b1:double,Dept_double_VectorAssembler_ee238b6831b1:double,Temperature:double,Fuel_Price:double,CPI_double_VectorAssembler_ee238b6831b1:double,Unemployment:double,Type_double_VectorAssembler_ee238b6831b1:double,Size_double_VectorAssembler_ee238b6831b1:double,Year_double_VectorAssembler_ee238b6831b1:double,Month_double_VectorAssembler_ee238b6831b1:double,Week_double_VectorAssembler_ee238b6831b1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 15 more\n",
      "23/02/04 04:55:51 ERROR Executor: Exception in task 0.0 in stage 53.0 (TID 83)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3838/0x000000084149f840: (struct<Store_double_VectorAssembler_ee238b6831b1:double,IsHoliday_double_VectorAssembler_ee238b6831b1:double,Dept_double_VectorAssembler_ee238b6831b1:double,Temperature:double,Fuel_Price:double,CPI_double_VectorAssembler_ee238b6831b1:double,Unemployment:double,Type_double_VectorAssembler_ee238b6831b1:double,Size_double_VectorAssembler_ee238b6831b1:double,Year_double_VectorAssembler_ee238b6831b1:double,Month_double_VectorAssembler_ee238b6831b1:double,Week_double_VectorAssembler_ee238b6831b1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 15 more\n",
      "23/02/04 04:55:51 WARN TaskSetManager: Lost task 0.0 in stage 53.0 (TID 83) (9878222aad48 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3838/0x000000084149f840: (struct<Store_double_VectorAssembler_ee238b6831b1:double,IsHoliday_double_VectorAssembler_ee238b6831b1:double,Dept_double_VectorAssembler_ee238b6831b1:double,Temperature:double,Fuel_Price:double,CPI_double_VectorAssembler_ee238b6831b1:double,Unemployment:double,Type_double_VectorAssembler_ee238b6831b1:double,Size_double_VectorAssembler_ee238b6831b1:double,Year_double_VectorAssembler_ee238b6831b1:double,Month_double_VectorAssembler_ee238b6831b1:double,Week_double_VectorAssembler_ee238b6831b1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 15 more\n",
      "\n",
      "23/02/04 04:55:51 ERROR TaskSetManager: Task 0 in stage 53.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o328.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 53.0 failed 1 times, most recent failure: Lost task 0.0 in stage 53.0 (TID 83) (9878222aad48 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3838/0x000000084149f840: (struct<Store_double_VectorAssembler_ee238b6831b1:double,IsHoliday_double_VectorAssembler_ee238b6831b1:double,Dept_double_VectorAssembler_ee238b6831b1:double,Temperature:double,Fuel_Price:double,CPI_double_VectorAssembler_ee238b6831b1:double,Unemployment:double,Type_double_VectorAssembler_ee238b6831b1:double,Size_double_VectorAssembler_ee238b6831b1:double,Year_double_VectorAssembler_ee238b6831b1:double,Month_double_VectorAssembler_ee238b6831b1:double,Week_double_VectorAssembler_ee238b6831b1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3838/0x000000084149f840: (struct<Store_double_VectorAssembler_ee238b6831b1:double,IsHoliday_double_VectorAssembler_ee238b6831b1:double,Dept_double_VectorAssembler_ee238b6831b1:double,Temperature:double,Fuel_Price:double,CPI_double_VectorAssembler_ee238b6831b1:double,Unemployment:double,Type_double_VectorAssembler_ee238b6831b1:double,Size_double_VectorAssembler_ee238b6831b1:double,Year_double_VectorAssembler_ee238b6831b1:double,Month_double_VectorAssembler_ee238b6831b1:double,Week_double_VectorAssembler_ee238b6831b1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prediction\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o328.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 53.0 failed 1 times, most recent failure: Lost task 0.0 in stage 53.0 (TID 83) (9878222aad48 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3838/0x000000084149f840: (struct<Store_double_VectorAssembler_ee238b6831b1:double,IsHoliday_double_VectorAssembler_ee238b6831b1:double,Dept_double_VectorAssembler_ee238b6831b1:double,Temperature:double,Fuel_Price:double,CPI_double_VectorAssembler_ee238b6831b1:double,Unemployment:double,Type_double_VectorAssembler_ee238b6831b1:double,Size_double_VectorAssembler_ee238b6831b1:double,Year_double_VectorAssembler_ee238b6831b1:double,Month_double_VectorAssembler_ee238b6831b1:double,Week_double_VectorAssembler_ee238b6831b1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$3838/0x000000084149f840: (struct<Store_double_VectorAssembler_ee238b6831b1:double,IsHoliday_double_VectorAssembler_ee238b6831b1:double,Dept_double_VectorAssembler_ee238b6831b1:double,Temperature:double,Fuel_Price:double,CPI_double_VectorAssembler_ee238b6831b1:double,Unemployment:double,Type_double_VectorAssembler_ee238b6831b1:double,Size_double_VectorAssembler_ee238b6831b1:double,Year_double_VectorAssembler_ee238b6831b1:double,Month_double_VectorAssembler_ee238b6831b1:double,Week_double_VectorAssembler_ee238b6831b1:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
