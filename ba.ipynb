{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZpFE004pn0zy"
   },
   "outputs": [],
   "source": [
    "# !pip install mxnet-mkl==1.6.0 numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qa9n4VWXoTgr",
    "outputId": "b5b87e93-916e-41a3-ebc1-6a273aae04d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elephas in /opt/bitnami/python/lib/python3.6/site-packages (3.4.3)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install elephas\n",
    "\n",
    "# exit() # restart the runtime to refresh installed pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Box_Ihr3oDDW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xXiqb_dkn0z5"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fE3jK2M5J07n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJPAjdP3J0vE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2rRSDgsomPx"
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"data/ba-walmart.zip\") as zf:\n",
    "#     zf.extractall(\n",
    "#         \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KnqYrJ4Kn0z7"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E_DNqmTyn0z8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 13:15:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Vu dep trai\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EC-LvMMgIn77"
   },
   "outputs": [],
   "source": [
    "sc= spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "us1moTa6TEHC"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/ba-walmart/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/ba-walmart/test.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/ba-walmart/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/ba-walmart/features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/ba-walmart/test.csv'"
     ]
    }
   ],
   "source": [
    "# test = pd.read_csv('data/ba-walmart/test.csv')\n",
    "# train = pd.read_csv('data/ba-walmart/train.csv')\n",
    "# features = pd.read_csv('data/ba-walmart/features.csv')\n",
    "# stores = pd.read_csv('data/ba-walmart/stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "snZ7yoy1Ti8W",
    "outputId": "a12e60a9-d5b4-4a34-d43d-f883f46731c1"
   },
   "outputs": [],
   "source": [
    "# features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nt6UeV9Rn0z-",
    "outputId": "b6986245-00cb-4237-fab4-7e54012bdb24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|Store|Type|  Size|\n",
      "+-----+----+------+\n",
      "|    1|   A|151315|\n",
      "|    2|   A|202307|\n",
      "|    3|   B| 37392|\n",
      "|    4|   A|205863|\n",
      "|    5|   B| 34875|\n",
      "+-----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stores_raw = spark.read.csv(\"data/ba-walmart/stores.csv\", header=True, inferSchema=True)\n",
    "df_stores_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Pk7jX0zn0z_",
    "outputId": "fa27aa50-77e6-4b83-87db-d79e416c9bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "|Store|               Date|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|        CPI|Unemployment|IsHoliday|\n",
      "+-----+-------------------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "|    1|2010-02-05 00:00:00|      42.31|     2.572|       NA|       NA|       NA|       NA|       NA|211.0963582|       8.106|    false|\n",
      "|    1|2010-02-12 00:00:00|      38.51|     2.548|       NA|       NA|       NA|       NA|       NA|211.2421698|       8.106|     true|\n",
      "|    1|2010-02-19 00:00:00|      39.93|     2.514|       NA|       NA|       NA|       NA|       NA|211.2891429|       8.106|    false|\n",
      "|    1|2010-02-26 00:00:00|      46.63|     2.561|       NA|       NA|       NA|       NA|       NA|211.3196429|       8.106|    false|\n",
      "|    1|2010-03-05 00:00:00|       46.5|     2.625|       NA|       NA|       NA|       NA|       NA|211.3501429|       8.106|    false|\n",
      "+-----+-------------------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_feature_raw = spark.read.csv(\"data/ba-walmart/features.csv\", header=True, inferSchema=True)\n",
    "df_feature_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZASv1J-n00A",
    "outputId": "77b04702-4f96-481c-f267-5ea7eb551ab6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------------+------------+---------+\n",
      "|Store|Dept|               Date|Weekly_Sales|IsHoliday|\n",
      "+-----+----+-------------------+------------+---------+\n",
      "|    1|   1|2010-02-05 00:00:00|     24924.5|    false|\n",
      "|    1|   1|2010-02-12 00:00:00|    46039.49|     true|\n",
      "|    1|   1|2010-02-19 00:00:00|    41595.55|    false|\n",
      "|    1|   1|2010-02-26 00:00:00|    19403.54|    false|\n",
      "|    1|   1|2010-03-05 00:00:00|     21827.9|    false|\n",
      "+-----+----+-------------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_raw = spark.read.csv(\"data/ba-walmart/train.csv\", header=True, inferSchema=True)\n",
    "df_test_raw = spark.read.csv(\"data/ba-walmart/test.csv\", header=True, inferSchema=True)\n",
    "df_train_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCL2kHWDn00B",
    "outputId": "301d6843-53b6-4ede-f6d4-9866c29d3a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------+----------+-----------+------------+---------+\n",
      "|Store|               Date|Temperature|Fuel_Price|        CPI|Unemployment|IsHoliday|\n",
      "+-----+-------------------+-----------+----------+-----------+------------+---------+\n",
      "|    1|2010-02-05 00:00:00|      42.31|     2.572|211.0963582|       8.106|    false|\n",
      "|    1|2010-02-12 00:00:00|      38.51|     2.548|211.2421698|       8.106|     true|\n",
      "|    1|2010-02-19 00:00:00|      39.93|     2.514|211.2891429|       8.106|    false|\n",
      "|    1|2010-02-26 00:00:00|      46.63|     2.561|211.3196429|       8.106|    false|\n",
      "|    1|2010-03-05 00:00:00|       46.5|     2.625|211.3501429|       8.106|    false|\n",
      "+-----+-------------------+-----------+----------+-----------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_feature_raw drop Markdown\n",
    "df_feature = df_feature_raw.drop(\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\")\n",
    "df_feature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgb1kTnwn00C",
    "outputId": "4a086a4f-e96f-4553-cd92-7a08e1a8b5aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- Dept: integer (nullable = true)\n",
      " |-- Weekly_Sales: double (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- CPI: float (nullable = true)\n",
      " |-- Unemployment: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      "\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+\n",
      "|Store|               Date|IsHoliday|Dept|Weekly_Sales|Temperature|Fuel_Price|      CPI|Unemployment|Type|  Size|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+\n",
      "|    1|2010-02-05 00:00:00|    false|   1|     24924.5|      42.31|     2.572|211.09636|       8.106|   A|151315|\n",
      "|    1|2010-02-12 00:00:00|     true|   1|    46039.49|      38.51|     2.548|211.24217|       8.106|   A|151315|\n",
      "|    1|2010-02-19 00:00:00|    false|   1|    41595.55|      39.93|     2.514|211.28914|       8.106|   A|151315|\n",
      "|    1|2010-02-26 00:00:00|    false|   1|    19403.54|      46.63|     2.561|211.31964|       8.106|   A|151315|\n",
      "|    1|2010-03-05 00:00:00|    false|   1|     21827.9|       46.5|     2.625|211.35014|       8.106|   A|151315|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge df_train and df_feature, df_store, join left\n",
    "df_train = df_train_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"], ).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "df_test = df_test_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"]).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "# convert CPI, Unemployment to float\n",
    "df_train = df_train.withColumn(\"CPI\", df_train[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df_train[\"Unemployment\"].cast(DoubleType()))\n",
    "df_test = df_test.withColumn(\"CPI\", df_test[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df_test[\"Unemployment\"].cast(DoubleType()))\n",
    "# summary info df_train\n",
    "df_train.printSchema()\n",
    "df_train.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdaZbMsfn00E",
    "outputId": "d7b6e8df-c0e8-486e-cba7-37dfb0641022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "|Store|               Date|IsHoliday|Dept|Weekly_Sales|Temperature|Fuel_Price|      CPI|Unemployment|Type|  Size|Year|Month|Week|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "|    1|2010-02-05 00:00:00|        0|   1|     24924.5|      42.31|     2.572|211.09636|       8.106|   A|151315|2010|    2|   5|\n",
      "|    1|2010-02-12 00:00:00|        1|   1|    46039.49|      38.51|     2.548|211.24217|       8.106|   A|151315|2010|    2|   6|\n",
      "|    1|2010-02-19 00:00:00|        0|   1|    41595.55|      39.93|     2.514|211.28914|       8.106|   A|151315|2010|    2|   7|\n",
      "|    1|2010-02-26 00:00:00|        0|   1|    19403.54|      46.63|     2.561|211.31964|       8.106|   A|151315|2010|    2|   8|\n",
      "|    1|2010-03-05 00:00:00|        0|   1|     21827.9|       46.5|     2.625|211.35014|       8.106|   A|151315|2010|    3|   9|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df_test = df_test.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df_train = df_train.withColumn(\"IsHoliday\", df_train[\"IsHoliday\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"IsHoliday\", df_test[\"IsHoliday\"].cast(IntegerType()))\n",
    "df_train.createOrReplaceTempView(\"df_train\")\n",
    "df_test.createOrReplaceTempView(\"df_test\")\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CGxh857En00F"
   },
   "outputs": [],
   "source": [
    "# # draw pie chart of store type given by df_stores\n",
    "# df_stores = df_stores_raw.toPandas()\n",
    "# df_stores[\"Type\"].value_counts().plot.pie(autopct=\"%1.1f%%\", figsize=(6, 6), labels=None).set_title(\"Store Type\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.legend(labels=df_stores[\"Type\"].value_counts().index, loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BStsy-59n00G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/bitnami/spark/python/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "df_train_pd = df_train.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCyhxLf6Sl_S",
    "outputId": "7c0a369c-17d2-46f3-eb0b-c7aa10bbc76f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store           0\n",
       "Date            0\n",
       "IsHoliday       0\n",
       "Dept            0\n",
       "Weekly_Sales    0\n",
       "Temperature     0\n",
       "Fuel_Price      0\n",
       "CPI             0\n",
       "Unemployment    0\n",
       "Type            0\n",
       "Size            0\n",
       "Year            0\n",
       "Month           0\n",
       "Week            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "g66Iqs47n00G"
   },
   "outputs": [],
   "source": [
    "#sns.scatterplot(x=\"Size\", y=\"Weekly_Sales\", data=df_train_pd, hue=\"Type\", alpha=0.2).set_title(\"Size, Type vs Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EzVMzfTtn00H"
   },
   "outputs": [],
   "source": [
    "# df_train_pd.groupby(\"IsHoliday\")[\"Weekly_Sales\"].mean().plot.bar().set_title(\"IsHoliday vs Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "j7boUuZ3n00I"
   },
   "outputs": [],
   "source": [
    "# sns.scatterplot(x=\"Fuel_Price\", y=\"Weekly_Sales\", hue=\"Type\", data=df_train_pd, alpha=0.2).set_title(\"Fuel_Price vs Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pP-1RQein00I"
   },
   "outputs": [],
   "source": [
    "# find relationship between Unemployment and Weekly_Sales\n",
    "# sns.scatterplot(x=\"Unemployment\", y=\"Weekly_Sales\", hue=\"Type\", data=df_train_pd, alpha=0.2).set_title(\"Unemployment vs Weekly_Sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Kvn7Hqbmn00J"
   },
   "outputs": [],
   "source": [
    "# find relationship between CPI and Weekly_Sales\n",
    "# sns.scatterplot(x=\"CPI\", y=\"Weekly_Sales\", hue=\"Type\", data=df_train_pd, alpha=0.2).set_title(\"CPI vs Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "o291Bva8n00J"
   },
   "outputs": [],
   "source": [
    "# find relationship between Temperature and Weekly_Sales\n",
    "# sns.scatterplot(x=\"Temperature\", y=\"Weekly_Sales\", hue=\"Type\", data=df_train_pd, alpha=0.2).set_title(\"Temperature vs Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uBLbMcUTn00K"
   },
   "outputs": [],
   "source": [
    "# average_sales_by_time = spark.sql(\"SELECT Year, Month, AVG(Weekly_Sales) AS Average_Sales FROM df_train GROUP BY Year, Month ORDER BY Year, Month\")\n",
    "# average_sales_by_time_pd = average_sales_by_time.toPandas()\n",
    "# sns.barplot(x=\"Month\", y=\"Average_Sales\", hue='Year', data=average_sales_by_time_pd).set_title(\"Average Sales by Month and Year\")\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKpbmyc-n00L",
    "outputId": "f3fb87b1-20c1-4b05-b3d7-137cf8afb398"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/bitnami/spark/python/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "|Store|               Date|IsHoliday|Dept|Weekly_Sales|Temperature|Fuel_Price|      CPI|Unemployment|Type|  Size|Year|Month|Week|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "|    1|2010-02-05 00:00:00|        0|   1|     24924.5|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|\n",
      "|    1|2010-02-12 00:00:00|        1|   1|    46039.49|      38.51|     2.548|211.24217|       8.106|   0|151315|2010|    2|   6|\n",
      "|    1|2010-02-19 00:00:00|        0|   1|    41595.55|      39.93|     2.514|211.28914|       8.106|   0|151315|2010|    2|   7|\n",
      "|    1|2010-02-26 00:00:00|        0|   1|    19403.54|      46.63|     2.561|211.31964|       8.106|   0|151315|2010|    2|   8|\n",
      "|    1|2010-03-05 00:00:00|        0|   1|     21827.9|       46.5|     2.625|211.35014|       8.106|   0|151315|2010|    3|   9|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "types = df_train.select(\"Type\").distinct().collect()\n",
    "types.sort()\n",
    "mapping = {t.Type: str(i) for i, t in enumerate(types)}\n",
    "df_train = df_train.replace(mapping, subset=[\"Type\"])\n",
    "df_test = df_test.replace(mapping, subset=[\"Type\"])\n",
    "df_train = df_train.withColumn(\"Type\", df_train[\"Type\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"Type\", df_test[\"Type\"].cast(IntegerType()))\n",
    "df_train_pd = df_train.toPandas()\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtWRt3gvn00L",
    "outputId": "928342ab-a9be-4cb7-a498-d673ef7201ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50/3010913235.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  results = df_train_pd.corr()[\"Weekly_Sales\"].sort_values(ascending=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly_Sales    1.000000\n",
      "Size            0.243828\n",
      "Dept            0.148032\n",
      "Month           0.028409\n",
      "Week            0.027673\n",
      "IsHoliday       0.012774\n",
      "Fuel_Price     -0.000120\n",
      "Temperature    -0.002312\n",
      "Year           -0.010111\n",
      "CPI            -0.020921\n",
      "Unemployment   -0.025864\n",
      "Store          -0.085195\n",
      "Type           -0.182242\n",
      "Name: Weekly_Sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# find correlation between attributes and Weekly_Sales\n",
    "results = df_train_pd.corr()[\"Weekly_Sales\"].sort_values(ascending=False)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAvlAQVcn00M",
    "outputId": "6346071d-d12e-4ea4-b573-5b581f76f6e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training attribute:  ['Weekly_Sales' 'Size' 'Dept' 'Month' 'Week' 'IsHoliday' 'Year' 'CPI'\n",
      " 'Unemployment' 'Store' 'Type']\n",
      "Ignore attribute:  {'Temperature', 'Date', 'Fuel_Price'}\n"
     ]
    }
   ],
   "source": [
    "keep_cols = results[results.abs() > 0.01].index.values\n",
    "print(\"Training attribute: \", keep_cols)\n",
    "ignore_attributes = set(df_train_pd.columns) - set(keep_cols)\n",
    "print(\"Ignore attribute: \", ignore_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "n6ijk8PVn00M"
   },
   "outputs": [],
   "source": [
    "df_train_clean = df_train.drop(*ignore_attributes)\n",
    "df_test_clean = df_test.drop(*ignore_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TO9SO_Qk4RME",
    "outputId": "9f3b20dc-2609-40b6-ac7e-e353300f1dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----+------------+---------+------------+----+------+----+-----+----+\n",
      "|Store|IsHoliday|Dept|Weekly_Sales|CPI      |Unemployment|Type|Size  |Year|Month|Week|\n",
      "+-----+---------+----+------------+---------+------------+----+------+----+-----+----+\n",
      "|1    |0        |1   |24924.5     |211.09636|8.106       |0   |151315|2010|2    |5   |\n",
      "|1    |1        |1   |46039.49    |211.24217|8.106       |0   |151315|2010|2    |6   |\n",
      "|1    |0        |1   |41595.55    |211.28914|8.106       |0   |151315|2010|2    |7   |\n",
      "|1    |0        |1   |19403.54    |211.31964|8.106       |0   |151315|2010|2    |8   |\n",
      "|1    |0        |1   |21827.9     |211.35014|8.106       |0   |151315|2010|3    |9   |\n",
      "|1    |0        |1   |21043.39    |211.38065|8.106       |0   |151315|2010|3    |10  |\n",
      "|1    |0        |1   |22136.64    |211.21564|8.106       |0   |151315|2010|3    |11  |\n",
      "|1    |0        |1   |26229.21    |211.01804|8.106       |0   |151315|2010|3    |12  |\n",
      "|1    |0        |1   |57258.43    |210.82045|7.808       |0   |151315|2010|4    |13  |\n",
      "|1    |0        |1   |42960.91    |210.62286|7.808       |0   |151315|2010|4    |14  |\n",
      "|1    |0        |1   |17596.96    |210.4887 |7.808       |0   |151315|2010|4    |15  |\n",
      "|1    |0        |1   |16145.35    |210.43912|7.808       |0   |151315|2010|4    |16  |\n",
      "|1    |0        |1   |16555.11    |210.38954|7.808       |0   |151315|2010|4    |17  |\n",
      "|1    |0        |1   |17413.94    |210.33997|7.808       |0   |151315|2010|5    |18  |\n",
      "|1    |0        |1   |18926.74    |210.33743|7.808       |0   |151315|2010|5    |19  |\n",
      "|1    |0        |1   |14773.04    |210.6171 |7.808       |0   |151315|2010|5    |20  |\n",
      "|1    |0        |1   |15580.43    |210.89676|7.808       |0   |151315|2010|5    |21  |\n",
      "|1    |0        |1   |17558.09    |211.17642|7.808       |0   |151315|2010|6    |22  |\n",
      "|1    |0        |1   |16637.62    |211.4561 |7.808       |0   |151315|2010|6    |23  |\n",
      "|1    |0        |1   |16216.27    |211.45377|7.808       |0   |151315|2010|6    |24  |\n",
      "|1    |0        |1   |16328.72    |211.33865|7.808       |0   |151315|2010|6    |25  |\n",
      "|1    |0        |1   |16333.14    |211.22353|7.787       |0   |151315|2010|7    |26  |\n",
      "|1    |0        |1   |17688.76    |211.10841|7.787       |0   |151315|2010|7    |27  |\n",
      "|1    |0        |1   |17150.84    |211.10039|7.787       |0   |151315|2010|7    |28  |\n",
      "|1    |0        |1   |15360.45    |211.23514|7.787       |0   |151315|2010|7    |29  |\n",
      "|1    |0        |1   |15381.82    |211.3699 |7.787       |0   |151315|2010|7    |30  |\n",
      "|1    |0        |1   |17508.41    |211.50467|7.787       |0   |151315|2010|8    |31  |\n",
      "|1    |0        |1   |15536.4     |211.63942|7.787       |0   |151315|2010|8    |32  |\n",
      "|1    |0        |1   |15740.13    |211.60336|7.787       |0   |151315|2010|8    |33  |\n",
      "|1    |0        |1   |15793.87    |211.5673 |7.787       |0   |151315|2010|8    |34  |\n",
      "|1    |0        |1   |16241.78    |211.53125|7.787       |0   |151315|2010|9    |35  |\n",
      "|1    |1        |1   |18194.74    |211.4952 |7.787       |0   |151315|2010|9    |36  |\n",
      "|1    |0        |1   |19354.23    |211.52246|7.787       |0   |151315|2010|9    |37  |\n",
      "|1    |0        |1   |18122.52    |211.59723|7.787       |0   |151315|2010|9    |38  |\n",
      "|1    |0        |1   |20094.19    |211.672  |7.838       |0   |151315|2010|10   |39  |\n",
      "|1    |0        |1   |23388.03    |211.74675|7.838       |0   |151315|2010|10   |40  |\n",
      "|1    |0        |1   |26978.34    |211.81375|7.838       |0   |151315|2010|10   |41  |\n",
      "|1    |0        |1   |25543.04    |211.8613 |7.838       |0   |151315|2010|10   |42  |\n",
      "|1    |0        |1   |38640.93    |211.90884|7.838       |0   |151315|2010|10   |43  |\n",
      "|1    |0        |1   |34238.88    |211.95639|7.838       |0   |151315|2010|11   |44  |\n",
      "|1    |0        |1   |19549.39    |212.00394|7.838       |0   |151315|2010|11   |45  |\n",
      "|1    |0        |1   |19552.84    |211.88968|7.838       |0   |151315|2010|11   |46  |\n",
      "|1    |1        |1   |18820.29    |211.74843|7.838       |0   |151315|2010|11   |47  |\n",
      "|1    |0        |1   |22517.56    |211.6072 |7.838       |0   |151315|2010|12   |48  |\n",
      "|1    |0        |1   |31497.65    |211.46596|7.838       |0   |151315|2010|12   |49  |\n",
      "|1    |0        |1   |44912.86    |211.40532|7.838       |0   |151315|2010|12   |50  |\n",
      "|1    |0        |1   |55931.23    |211.40512|7.838       |0   |151315|2010|12   |51  |\n",
      "|1    |1        |1   |19124.58    |211.40494|7.838       |0   |151315|2010|12   |52  |\n",
      "|1    |0        |1   |15984.24    |211.40474|7.742       |0   |151315|2011|1    |1   |\n",
      "|1    |0        |1   |17359.7     |211.45741|7.742       |0   |151315|2011|1    |2   |\n",
      "+-----+---------+----+------------+---------+------------+----+------+----+-----+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_clean.show(50, truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ptx_3p9x0pc9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (trainingDF, validationDF) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcast(DoubleType())\u001b[38;5;241m+\u001b[39mlit(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# (trainingDF, validationDF) = train_df.withColumn(\"label\", train_df[\"class\"].cast(DoubleType())+lit(1)).select(\"features\", \"label\").randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vqXsmiZ0eQR"
   },
   "outputs": [],
   "source": [
    "# train_df.withColumn(\"label\", train_df[\"class\"].cast(DoubleType())+lit(1)).select(\"features\", \"label\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ini6zfGzrdN"
   },
   "outputs": [],
   "source": [
    "# trainingDF.show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fQCwS_4vAPc1"
   },
   "outputs": [],
   "source": [
    "df_train =df_train.drop(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TCkWR9v5n00N"
   },
   "outputs": [],
   "source": [
    "# create regression model\n",
    "input_cols = df_train.columns\n",
    "# input_cols = ['CPI', 'Unemployment', 'IsHoliday', 'Week','Weekly_Sales']\n",
    "input_cols.remove(\"Weekly_Sales\")\n",
    "output_col = \"Weekly_Sales\"\n",
    "assembler = VectorAssembler( outputCol=\"preFeatures\")\n",
    "assembler.setInputCols(input_cols)\n",
    "#df_train-_clean_ = df_train_clean_.withColumn(\"weight\", when(df_train_clean_[\"IsHoliday\"] == 1, 5).otherwise(1))\n",
    "assemble_train_df = assembler.transform(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ZIoHjpLC0TGq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df_train_clean_, df_val) = assemble_train_df.select(\"preFeatures\", \"Weekly_Sales\").randomSplit([0.8, 0.2], seed=42)\n",
    "scaler = MinMaxScaler(inputCol=\"preFeatures\", outputCol=\"features\")\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scaler.fit(df_train_clean_).transform(df_train_clean_).select(\"features\", \"Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "25FXdeCwLJFQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "# from tensorflow.keras.utils import to_categorical, generic_utils\n",
    "\n",
    "# nb_classes = train_df.select(\"category\").distinct().count()\n",
    "input_dim = len(scaledData.select(\"features\").first()[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_shape=(input_dim,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "zgJeOWB7LNI6",
    "outputId": "25dff2b8-22a7-48bc-fd3c-cf4982ff8abe"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ElephasEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize SparkML Estimator and set all relevant properties\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[43mElephasEstimator\u001b[49m()\n\u001b[1;32m      3\u001b[0m estimator\u001b[38;5;241m.\u001b[39msetFeaturesCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)             \u001b[38;5;66;03m# These two come directly from pyspark,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m estimator\u001b[38;5;241m.\u001b[39msetLabelCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_category\u001b[39m\u001b[38;5;124m\"\u001b[39m)                 \u001b[38;5;66;03m# hence the camel case. Sorry :)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ElephasEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Initialize SparkML Estimator and set all relevant properties\n",
    "# estimator = ElephasEstimator()\n",
    "# estimator.setFeaturesCol(\"scaled_features\")             # These two come directly from pyspark,\n",
    "# estimator.setLabelCol(\"index_category\")                 # hence the camel case. Sorry :)\n",
    "# estimator.set_keras_model_config(model.to_yaml())       # Provide serialized Keras model\n",
    "# estimator.set_categorical_labels(True)\n",
    "# estimator.set_nb_classes(nb_classes)\n",
    "# estimator.set_num_workers(1)  # We just use one worker here. Feel free to adapt it.\n",
    "# estimator.set_epochs(20) \n",
    "# estimator.set_batch_size(128)\n",
    "# estimator.set_verbosity(1)\n",
    "# estimator.set_validation_split(0.15)\n",
    "# estimator.set_optimizer_config(opt_conf)\n",
    "# estimator.set_mode(\"synchronous\")\n",
    "# estimator.set_loss(\"categorical_crossentropy\")\n",
    "# estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_U_r5J5Ohgn",
    "outputId": "f3ddd32b-66fc-45b0-8f66-6c72d9f02ea3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/bitnami/python/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_24be7e543d41"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elephas.ml_model import ElephasEstimator\n",
    "from tensorflow.keras import optimizers\n",
    "# from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n",
    "# Initialize Spark ML Estimator\n",
    "estimator = ElephasEstimator()\n",
    "estimator.set_keras_model_config(model.to_json())\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.setLabelCol(\"Weekly_Sales\") \n",
    "estimator.set_mode(\"synchronous\")\n",
    "estimator.set_loss(\"mae\")\n",
    "estimator.set_metrics(['mse'])\n",
    "estimator.set_epochs(5)\n",
    "estimator.set_batch_size(32)\n",
    "estimator.set_validation_split(0.1)\n",
    "estimator.set_categorical_labels(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gy64Xw5-LU9x",
    "outputId": "7b0d2775-5f94-48d8-933a-6639b2f84bce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 13:29:08.081326: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:08.090765: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:08.093559: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:08.111498: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:08.132939: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:08.141761: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:08.258072: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:08.272849: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:08.459873: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:08.459934: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-31 13:29:08.483681: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:08.483748: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-31 13:29:08.516923: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:08.517125: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-31 13:29:08.549722: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-31 13:29:08.615640: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-31 13:29:08.616203: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:08.616306: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-31 13:29:08.629695: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-31 13:29:08.646262: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:08.646985: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-31 13:29:08.675409: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:08.675483: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-31 13:29:08.734178: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-31 13:29:08.739639: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:08.739718: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-31 13:29:08.775650: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-31 13:29:08.824320: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-31 13:29:08.875128: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-31 13:29:08.896741: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:08.896951: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-31 13:29:08.998325: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 13:29:10.580606: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:10.583242: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:10.583425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-31 13:29:10.633067: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:10.634630: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:10.635002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-31 13:29:10.764095: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:10.765974: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:10.766229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-31 13:29:10.805697: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:10.807916: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:10.808236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-31 13:29:10.899382: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:10.901680: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:10.901893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-31 13:29:11.295518: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:11.298001: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:11.298221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-31 13:29:11.601306: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:11.604550: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:11.604759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-31 13:29:11.625281: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:11.626304: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:11.626477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-31 13:29:13.894233: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:13.894478: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-31 13:29:13.894606: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bea5171a16db): /proc/driver/nvidia/version does not exist\n",
      "2023-01-31 13:29:13.895303: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:14.031664: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:14.031908: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-31 13:29:14.032027: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bea5171a16db): /proc/driver/nvidia/version does not exist\n",
      "2023-01-31 13:29:14.033078: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 13:29:14.111045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:14.111245: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-31 13:29:14.111328: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bea5171a16db): /proc/driver/nvidia/version does not exist\n",
      "2023-01-31 13:29:14.112063: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:14.488069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:14.488306: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-31 13:29:14.488392: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bea5171a16db): /proc/driver/nvidia/version does not exist\n",
      "2023-01-31 13:29:14.489169: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:15.072650: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:15.072732: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-31 13:29:15.072772: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bea5171a16db): /proc/driver/nvidia/version does not exist\n",
      "2023-01-31 13:29:15.073404: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:15.773069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:15.773155: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-31 13:29:15.773195: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bea5171a16db): /proc/driver/nvidia/version does not exist\n",
      "2023-01-31 13:29:15.773836: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:15.802663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:15.802860: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-31 13:29:15.802945: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bea5171a16db): /proc/driver/nvidia/version does not exist\n",
      "2023-01-31 13:29:15.803648: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:29:15.848894: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bitnami/python/lib:/opt/bitnami/spark/venv/lib/python3.8/site-packages/numpy.libs:\n",
      "2023-01-31 13:29:15.849400: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-31 13:29:15.849756: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bea5171a16db): /proc/driver/nvidia/version does not exist\n",
      "2023-01-31 13:29:15.850802: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[Stage 53:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Synchronous training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fitted_pipeline = estimator.fit(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhPE9f7f1slX"
   },
   "outputs": [],
   "source": [
    "changedTypedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3poUIZKxaez"
   },
   "outputs": [],
   "source": [
    "df_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PgrQ3iX77_Q"
   },
   "outputs": [],
   "source": [
    "df_train_clean.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_train_clean.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0L3sHuqwH4x"
   },
   "outputs": [],
   "source": [
    "del assemble_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6D1JmaaCn00N"
   },
   "outputs": [],
   "source": [
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rd8a5xzR4Ff-"
   },
   "outputs": [],
   "source": [
    "df_train_clean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wiln0drOTz9s"
   },
   "outputs": [],
   "source": [
    "scaledData.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_dOs-0Zw9oQ"
   },
   "outputs": [],
   "source": [
    "len(input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aykXpch91lY7"
   },
   "outputs": [],
   "source": [
    "df_train_clean_.count(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrcrYxuz3Ej5"
   },
   "outputs": [],
   "source": [
    "df_train_clean_['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gl1boO7wDfmG"
   },
   "outputs": [],
   "source": [
    "df_train_clean_.show(500,truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da1Vavob3Een"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cdcnc52TwBwe"
   },
   "outputs": [],
   "source": [
    "last = df_train_clean_.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f-eYwYMwcE5"
   },
   "outputs": [],
   "source": [
    "last.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reBO5um_zXMc"
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=output_col, maxIter=10, regParam=0.3, elasticNetParam=0.8, weightCol=\"weight\")\n",
    "pipeline =  pyspark.ml.Pipeline(stages=[assembler, lr])\n",
    "model = pipeline.fit(df_train_clean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URw_GIR1vu6Z"
   },
   "outputs": [],
   "source": [
    "type(assemble_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaDdAVgdn00P"
   },
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "df_val_pred = model.transform(df_val)\n",
    "df_val_pred = df_val_pred.withColumn(\"weight\", when(df_val_pred[\"IsHoliday\"] == 1, 5).otherwise(1))\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=output_col, metricName=\"mae\", weightCol=\"weight\")\n",
    "wmae = evaluator.evaluate(df_val_pred, {evaluator.metricName: \"mae\"})\n",
    "print(\"WMAE: \", wmae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IxJ7Pf9n00P"
   },
   "outputs": [],
   "source": [
    "df_val_pred.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eR3BqABrn00Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYhkTLkcLMvX"
   },
   "source": [
    "##### ***Download the data***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "devxHuDW-0Mb"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/jbrownleedatasets/master/pima-indians-diabetes.data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bni-rogFMQVV"
   },
   "source": [
    "\n",
    "DLlib supports Spark Dataframes as the input to the distributed training, and as the input/output of the distributed inference. Consequently, the user can easily process large-scale dataset using Apache Spark, and directly apply AI models on the distributed (and possibly in-memory) Dataframes without data conversion or serialization   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfwvZDqVQDEh"
   },
   "source": [
    "##### **Load the data into Spark dataframe using Spark API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVpFKkCX_3WF"
   },
   "outputs": [],
   "source": [
    "path = \"pima-indians-diabetes.data.csv\"\n",
    "df = spark.read.csv(path, sep=',', inferSchema=True).toDF(\"num_times_pregrant\", \"plasma_glucose\", \"blood_pressure\", \"skin_fold_thickness\", \"2-hour_insulin\", \"body_mass_index\", \"diabetes_pedigree_function\", \"age\", \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68UHw7AK-ouh"
   },
   "source": [
    "#### **Step 2: Process data using Spark DataFrame api** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_15SCI5_-LPi"
   },
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(outputCol=\"features\")\n",
    "vecAssembler.setInputCols([\"num_times_pregrant\", \"plasma_glucose\", \"blood_pressure\", \"skin_fold_thickness\", \"2-hour_insulin\", \"body_mass_index\", \"diabetes_pedigree_function\", \"age\"])\n",
    "train_df = vecAssembler.transform(df)\n",
    "\n",
    "changedTypedf = train_df.withColumn(\"label\", train_df[\"class\"].cast(DoubleType())+lit(1))\\\n",
    "    .select(\"features\", \"label\")\n",
    "(trainingDF, validationDF) = changedTypedf.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOzcmQoF18pM"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ld-pV8e-Vb7D"
   },
   "source": [
    "#### **Step 3: Define model using DLlib keras-style api** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAMxwh-KBrdt"
   },
   "outputs": [],
   "source": [
    "x1 = Input(shape=(8,))\n",
    "dense1 = Dense(12, activation='relu')(x1)\n",
    "dense2 = Dense(8, activation='relu')(dense1)\n",
    "dense3 = Dense(2)(dense2)\n",
    "model = Model(x1, dense3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kEZlexfMm0z"
   },
   "source": [
    "#### **Step 4: Create NNClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwgQBn-jHE0C"
   },
   "outputs": [],
   "source": [
    "x1 = Input(shape=(8,))\n",
    "dense1 = Dense(10, activation='relu')(x1)\n",
    "dense2 = Dense(1)(dense1)\n",
    "model = Model(x1, dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MwOMDijHE0C"
   },
   "outputs": [],
   "source": [
    "classifier = NNEstimator(model, MSECriterion()) \\\n",
    "    .setOptimMethod(Adam()) \\\n",
    "    .setBatchSize(32) \\\n",
    "    .setMaxEpoch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzgamNMmNPV3"
   },
   "source": [
    "Train the model with Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuNofnOR12r-"
   },
   "outputs": [],
   "source": [
    "trainingDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8hNcS9t12lu"
   },
   "outputs": [],
   "source": [
    "type(trainingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcW_0LJoPQMT"
   },
   "outputs": [],
   "source": [
    "train_df.withColumn(\"label\", train_df[\"class\"].cast(DoubleType())+lit(1)).show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhg8grSEV2FX"
   },
   "outputs": [],
   "source": [
    "changedTypedf.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQvw3n6LOk_2"
   },
   "outputs": [],
   "source": [
    "trainingDF.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Val33GtFQDbA"
   },
   "outputs": [],
   "source": [
    "changedTypedf.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4x-S30kNSRC"
   },
   "outputs": [],
   "source": [
    "nnModel = classifier.fit(trainingDF)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2a6db3fe0e1f17f9db6507cca1043594146b6ad8d5140d96aa4ff535d518e24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
