{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sparktorch import serialize_torch_obj, SparkTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 12:03:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Vu dep trai\").config(\"spark.executor.memory\",\"10g\").getOrCreate()\n",
    "# conf = pyspark.SparkConf().setMaster(\"spark://node-master:7077\")\\\n",
    "#         .setAppName(\"Vu dep trai\")\\\n",
    "#         .set(\"spark.executor.memory\",\"15g\")\n",
    "# # sc = SparkContext.getOrCreate(conf=conf)\n",
    "# # spark.stop()\n",
    "# sc = SparkContext(conf = conf)\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores_raw = spark.read.csv(\"data/ba-walmart/stores.csv\", header=True, inferSchema=True)\n",
    "df_feature_raw = spark.read.csv(\"data/ba-walmart/features.csv\", header=True, inferSchema=True)\n",
    "df_train_raw = spark.read.csv(\"data/ba-walmart/train.csv\", header=True, inferSchema=True)\n",
    "df_test_raw = spark.read.csv(\"data/ba-walmart/test.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_feature_raw.drop(\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\")\n",
    "df = df_train_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"], ).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "df_test = df_test_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"]).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "df = df.withColumn(\"CPI\", df[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df[\"Unemployment\"].cast(FloatType()))\n",
    "df_test = df_test.withColumn(\"CPI\", df_test[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df_test[\"Unemployment\"].cast(FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df_test = df_test.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df = df.withColumn(\"IsHoliday\", df[\"IsHoliday\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"IsHoliday\", df_test[\"IsHoliday\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.filter(df[\"Weekly_Sales\"] > 0)\n",
    "df_clean = df_clean.filter(df_clean[\"Weekly_Sales\"] < 450000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "types = df_clean.select(\"Type\").distinct().collect()\n",
    "types.sort()\n",
    "mapping = {t.Type: str(i) for i, t in enumerate(types)}\n",
    "df_clean = df_clean.replace(mapping, subset=[\"Type\"])\n",
    "df_test = df_test.replace(mapping, subset=[\"Type\"])\n",
    "df_clean = df_clean.withColumn(\"Type\", df_clean[\"Type\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"Type\", df_test[\"Type\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From EDA select important columns\n",
    "input_col = ['Store', 'IsHoliday', 'Type', 'Size', 'Week','Dept','Year']\n",
    "target = 'Weekly_Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 80% first data for training\n",
    "df_train, df_valid = df_clean.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_col, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(7, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "torch_obj = serialize_torch_obj(\n",
    "    model=net,\n",
    "    criterion=nn.MSELoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.0001\n",
    ")\n",
    "\n",
    "spark_model = SparkTorch(\n",
    "    inputCol='scaledFeatures',\n",
    "    labelCol=target,\n",
    "    predictionCol='predictions',\n",
    "    torchObj=torch_obj,\n",
    "    iters=2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition: 2a47eb9b-6e07-491f-8347-81a263ee2298. Iteration: 0. Distributed Loss: None Partition Training Loss: 549430144.0, Partition Validation Loss: None\n",
      "Partition: f4c48d52-e982-4e5c-b351-e53af77c92ca. Iteration: 0. Distributed Loss: None Partition Training Loss: 293181184.0, Partition Validation Loss: None\n",
      "Partition: 682b4bed-c9e4-4303-b657-18300f9b7000. Iteration: 0. Distributed Loss: None Partition Training Loss: 736884608.0, Partition Validation Loss: None\n",
      "Partition: 6e89eef3-5492-44d0-823e-169a66bc4bd5. Iteration: 0. Distributed Loss: None Partition Training Loss: 1049317376.0, Partition Validation Loss: None\n",
      "Partition: f4c48d52-e982-4e5c-b351-e53af77c92ca. Iteration: 1. Distributed Loss: None Partition Training Loss: 293181088.0, Partition Validation Loss: None\n",
      "Partition: 2a47eb9b-6e07-491f-8347-81a263ee2298. Iteration: 1. Distributed Loss: None Partition Training Loss: 549430016.0, Partition Validation Loss: None\n",
      "Partition: 682b4bed-c9e4-4303-b657-18300f9b7000. Iteration: 1. Distributed Loss: None Partition Training Loss: 736884480.0, Partition Validation Loss: None\n",
      "Partition: 6e89eef3-5492-44d0-823e-169a66bc4bd5. Iteration: 1. Distributed Loss: None Partition Training Loss: 1049317056.0, Partition Validation Loss: None\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "p = pyspark.ml.Pipeline(stages=[assembler, scaler, spark_model])\n",
    "model = p.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+--------------------+--------------------+-------------------+\n",
      "|Store|               Date|IsHoliday|Dept|Weekly_Sales|Temperature|Fuel_Price|      CPI|Unemployment|Type|  Size|Year|Month|Week|            features|      scaledFeatures|        predictions|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+--------------------+--------------------+-------------------+\n",
      "|    1|2010-02-05 00:00:00|        0|   2|    50605.27|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.15043896436691284|\n",
      "|    1|2010-02-05 00:00:00|        0|   5|    32229.38|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.14201930165290833|\n",
      "|    1|2010-02-05 00:00:00|        0|  23|    24146.49|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.11890837550163269|\n",
      "|    1|2010-02-05 00:00:00|        0|  26|    11737.12|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.11968255043029785|\n",
      "|    1|2010-02-05 00:00:00|        0|  27|      2293.0|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.11980000138282776|\n",
      "|    1|2010-02-05 00:00:00|        0|  28|     1085.29|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.11976505070924759|\n",
      "|    1|2010-02-05 00:00:00|        0|  29|     7024.95|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.11950847506523132|\n",
      "|    1|2010-02-05 00:00:00|        0|  38|   115564.35|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.11416640877723694|\n",
      "|    1|2010-02-05 00:00:00|        0|  49|    11501.46|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.10917054861783981|\n",
      "|    1|2010-02-05 00:00:00|        0|  52|     3508.04|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.10835466533899307|\n",
      "|    1|2010-02-05 00:00:00|        0|  59|     1409.34|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.10823270678520203|\n",
      "|    1|2010-02-05 00:00:00|        0|  72|    98499.12|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.12436249107122421|\n",
      "|    1|2010-02-05 00:00:00|        0|  74|    12251.94|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.13009297847747803|\n",
      "|    1|2010-02-05 00:00:00|        0|  79|    46729.77|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...| 0.1452077329158783|\n",
      "|    1|2010-02-05 00:00:00|        0|  85|     3825.78|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...| 0.1678539514541626|\n",
      "|    1|2010-02-05 00:00:00|        0|  91|    62424.14|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.19444707036018372|\n",
      "|    1|2010-02-05 00:00:00|        0|  97|    32153.04|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.21796710789203644|\n",
      "|    1|2010-02-05 00:00:00|        0|  98|    10891.37|      42.31|     2.572|211.09636|       8.106|   0|151315|2010|    2|   5|[1.0,0.0,0.0,1513...|[-1.6577908705381...|0.22127975523471832|\n",
      "|    1|2010-02-12 00:00:00|        1|   4|    35351.21|      38.51|     2.548|211.24217|       8.106|   0|151315|2010|    2|   6|[1.0,1.0,0.0,1513...|[-1.6577908705381...|0.41472768783569336|\n",
      "|    1|2010-02-12 00:00:00|        1|  12|     8654.07|      38.51|     2.548|211.24217|       8.106|   0|151315|2010|    2|   6|[1.0,1.0,0.0,1513...|[-1.6577908705381...|0.42056572437286377|\n",
      "+-----+-------------------+---------+----+------------+-----------+----------+---------+------------+----+------+----+-----+----+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pred = model.transform(df_valid)\n",
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  16070.635356372743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get mae\n",
    "evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"predictions\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(pred)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
