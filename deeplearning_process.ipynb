{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/bitnami/python/lib/python3.8/site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/bitnami/python/lib/python3.8/site-packages (from pyarrow) (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "# import pyspark.pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sparktorch import serialize_torch_obj, SparkTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 18:36:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Vu dep trai\").config(\"spark.executor.memory\",\"10g\").getOrCreate()\n",
    "# conf = pyspark.SparkConf().setMaster(\"spark://node-master:7077\")\\\n",
    "#         .setAppName(\"Vu dep trai\")\\\n",
    "#         .set(\"spark.executor.memory\",\"15g\")\n",
    "# # sc = SparkContext.getOrCreate(conf=conf)\n",
    "# # spark.stop()\n",
    "# sc = SparkContext(conf = conf)\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores_raw = spark.read.csv(\"data/ba-walmart/stores.csv\", header=True, inferSchema=True)\n",
    "df_feature_raw = spark.read.csv(\"data/ba-walmart/features.csv\", header=True, inferSchema=True)\n",
    "df_train_raw = spark.read.csv(\"data/ba-walmart/train.csv\", header=True, inferSchema=True)\n",
    "df_test_raw = spark.read.csv(\"data/ba-walmart/test.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_feature_raw.drop(\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\")\n",
    "df = df_train_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"], ).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "df_test = df_test_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"]).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "df = df.withColumn(\"CPI\", df[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df[\"Unemployment\"].cast(FloatType()))\n",
    "df_test = df_test.withColumn(\"CPI\", df_test[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df_test[\"Unemployment\"].cast(FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df_test = df_test.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df = df.withColumn(\"IsHoliday\", df[\"IsHoliday\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"IsHoliday\", df_test[\"IsHoliday\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.filter(df[\"Weekly_Sales\"] > 0)\n",
    "df_clean = df_clean.filter(df_clean[\"Weekly_Sales\"] < 450000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = df_clean.select(\"Type\").distinct().collect()\n",
    "types.sort()\n",
    "mapping = {t.Type: str(i) for i, t in enumerate(types)}\n",
    "df_clean = df_clean.replace(mapping, subset=[\"Type\"])\n",
    "df_test = df_test.replace(mapping, subset=[\"Type\"])\n",
    "df_clean = df_clean.withColumn(\"Type\", df_clean[\"Type\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"Type\", df_test[\"Type\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From EDA select important columns\n",
    "drop_col = ['Date', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Month']\n",
    "# input_col = ['Store', 'IsHoliday', 'Type', 'Size', 'Week','Dept','Year']\n",
    "onehot_col = ['Store', 'Type']\n",
    "target = 'Weekly_Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_scale = df_clean.agg({\"Weekly_Sales\": \"mean\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Size</th>\n",
       "      <th>Year</th>\n",
       "      <th>Week</th>\n",
       "      <th>Store_1</th>\n",
       "      <th>Store_2</th>\n",
       "      <th>Store_3</th>\n",
       "      <th>Store_4</th>\n",
       "      <th>...</th>\n",
       "      <th>Store_39</th>\n",
       "      <th>Store_40</th>\n",
       "      <th>Store_41</th>\n",
       "      <th>Store_42</th>\n",
       "      <th>Store_43</th>\n",
       "      <th>Store_44</th>\n",
       "      <th>Store_45</th>\n",
       "      <th>Type_0</th>\n",
       "      <th>Type_1</th>\n",
       "      <th>Type_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059020</td>\n",
       "      <td>0.630267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109019</td>\n",
       "      <td>0.630267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098496</td>\n",
       "      <td>0.630267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045947</td>\n",
       "      <td>0.630267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051687</td>\n",
       "      <td>0.630267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IsHoliday  Dept  Weekly_Sales      Size  Year      Week  Store_1  Store_2  \\\n",
       "0        0.0   0.0      0.059020  0.630267   0.0  0.078431      1.0      0.0   \n",
       "1        1.0   0.0      0.109019  0.630267   0.0  0.098039      1.0      0.0   \n",
       "2        0.0   0.0      0.098496  0.630267   0.0  0.117647      1.0      0.0   \n",
       "3        0.0   0.0      0.045947  0.630267   0.0  0.137255      1.0      0.0   \n",
       "4        0.0   0.0      0.051687  0.630267   0.0  0.156863      1.0      0.0   \n",
       "\n",
       "   Store_3  Store_4  ...  Store_39  Store_40  Store_41  Store_42  Store_43  \\\n",
       "0      0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "1      0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "2      0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "3      0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "4      0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   Store_44  Store_45  Type_0  Type_1  Type_2  \n",
       "0       0.0       0.0     1.0     0.0     0.0  \n",
       "1       0.0       0.0     1.0     0.0     0.0  \n",
       "2       0.0       0.0     1.0     0.0     0.0  \n",
       "3       0.0       0.0     1.0     0.0     0.0  \n",
       "4       0.0       0.0     1.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.drop(*drop_col)\n",
    "df_clean = df_clean.na.drop()\n",
    "df_clean_pd = df_clean.toPandas()\n",
    "min_target = df_clean_pd[target].min()\n",
    "max_target = df_clean_pd[target].max()\n",
    "for oh_cols in onehot_col:\n",
    "    df_clean_pd = pd.concat([df_clean_pd, pd.get_dummies(df_clean_pd[oh_cols], prefix=oh_cols)], axis=1)\n",
    "    df_clean_pd = df_clean_pd.drop(oh_cols, axis=1)\n",
    "    \n",
    "df_clean_pd = (df_clean_pd - df_clean_pd.min()) / (df_clean_pd.max() - df_clean_pd.min())\n",
    "df_clean_pd = df_clean_pd.dropna()\n",
    "df_clean_pd = df_clean_pd.reset_index(drop=True)\n",
    "df_clean_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/bitnami/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/bitnami/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "df_clean = spark.createDataFrame(df_clean_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scaler\n",
    "all_col = df_clean.columns\n",
    "all_col.remove(target)\n",
    "mm_assembler = VectorAssembler(inputCols=all_col, outputCol=\"features\")\n",
    "mm_pipeline = pyspark.ml.Pipeline(stages=[mm_assembler]).fit(df_clean)\n",
    "df_clean = mm_pipeline.transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 18:38:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/02/04 18:38:35 WARN TaskSetManager: Stage 16 contains a task of very large size (16633 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 18:38:39 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 16 (TID 24): Attempting to kill Python Worker\n",
      "+-------------------+\n",
      "|Weekly_Sales       |\n",
      "+-------------------+\n",
      "|0.05901994249481135|\n",
      "|0.10901918001495786|\n",
      "|0.0984961529339467 |\n",
      "|0.04594658606039068|\n",
      "|0.05168734897215822|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_clean[[\"Weekly_Sales\"]].show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 80% first data for training\n",
    "df_train, df_valid = df_clean.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(53, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "torch_obj = serialize_torch_obj(\n",
    "    model=net,\n",
    "    criterion=nn.MSELoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.0001\n",
    ")\n",
    "\n",
    "spark_model = SparkTorch(\n",
    "    inputCol='features',\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    torchObj=torch_obj,\n",
    "    miniBatch=1000,\n",
    "    iters=50,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 18:52:08 WARN TaskSetManager: Stage 20 contains a task of very large size (16633 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 18:52:13 ERROR Executor: Exception in task 0.0 in stage 20.0 (TID 72)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/bitnami/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/sparktorch/distributed.py\", line 254, in <lambda>\n",
      "    rdd, lambda i, x: handle_model(\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/sparktorch/distributed.py\", line 105, in handle_model\n",
      "    dist.init_process_group('gloo', rank=index + 1, world_size=world_size)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 503, in init_process_group\n",
      "    _update_default_pg(_new_process_group_helper(\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 588, in _new_process_group_helper\n",
      "    pg = ProcessGroupGloo(\n",
      "RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:769] connect [172.17.0.2]:24477: Connection refused\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/04 18:52:13 WARN TaskSetManager: Lost task 0.0 in stage 20.0 (TID 72) (c84f15796326 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/bitnami/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/sparktorch/distributed.py\", line 254, in <lambda>\n",
      "    rdd, lambda i, x: handle_model(\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/sparktorch/distributed.py\", line 105, in handle_model\n",
      "    dist.init_process_group('gloo', rank=index + 1, world_size=world_size)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 503, in init_process_group\n",
      "    _update_default_pg(_new_process_group_helper(\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 588, in _new_process_group_helper\n",
      "    pg = ProcessGroupGloo(\n",
      "RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:769] connect [172.17.0.2]:24477: Connection refused\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/02/04 18:52:13 WARN TaskSetManager: Lost task 2.0 in stage 20.0 (TID 63) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n",
      "23/02/04 18:52:13 WARN TaskSetManager: Lost task 11.0 in stage 20.0 (TID 62) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n",
      "23/02/04 18:52:13 WARN TaskSetManager: Lost task 5.0 in stage 20.0 (TID 64) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n",
      "23/02/04 18:52:13 WARN TaskSetManager: Lost task 6.0 in stage 20.0 (TID 71) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n",
      "23/02/04 18:52:13 WARN TaskSetManager: Lost task 3.0 in stage 20.0 (TID 70) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(20, 0) finished unsuccessfully.\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/bitnami/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/bitnami/python/lib/python3.8/site-packages/sparktorch/distributed.py\", line 254, in <lambda>\n    rdd, lambda i, x: handle_model(\n  File \"/opt/bitnami/python/lib/python3.8/site-packages/sparktorch/distributed.py\", line 105, in handle_model\n    dist.init_process_group('gloo', rank=index + 1, world_size=world_size)\n  File \"/opt/bitnami/python/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 503, in init_process_group\n    _update_default_pg(_new_process_group_helper(\n  File \"/opt/bitnami/python/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 588, in _new_process_group_helper\n    pg = ProcessGroupGloo(\nRuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:769] connect [172.17.0.2]:24477: Connection refused\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2857)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m p \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39mml\u001b[39m.\u001b[39mPipeline(stages\u001b[39m=\u001b[39m[spark_model])\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mfit(df_train)\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    135\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/sparktorch/torch_distributed.py:297\u001b[0m, in \u001b[0;36mSparkTorch._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m master_url \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39mgetConf()\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mspark.driver.host\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__str__\u001b[39m()\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msynchronous\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m     state_dict \u001b[39m=\u001b[39m train_distributed(\n\u001b[1;32m    298\u001b[0m         rdd\u001b[39m=\u001b[39;49mrdd,\n\u001b[1;32m    299\u001b[0m         torch_obj\u001b[39m=\u001b[39;49mtorch_obj,\n\u001b[1;32m    300\u001b[0m         iters\u001b[39m=\u001b[39;49miters,\n\u001b[1;32m    301\u001b[0m         partition_shuffles\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    302\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    303\u001b[0m         mini_batch\u001b[39m=\u001b[39;49mmini_batch,\n\u001b[1;32m    304\u001b[0m         validation_pct\u001b[39m=\u001b[39;49mvalidation_pct,\n\u001b[1;32m    305\u001b[0m         world_size\u001b[39m=\u001b[39;49mpartitions\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    306\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    307\u001b[0m         early_stop_patience\u001b[39m=\u001b[39;49mearly_stop_patience\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhogwild\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    312\u001b[0m     \u001b[39mif\u001b[39;00m barrier:\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/sparktorch/distributed.py:253\u001b[0m, in \u001b[0;36mtrain_distributed\u001b[0;34m(rdd, torch_obj, iters, partition_shuffles, verbose, mini_batch, validation_pct, world_size, device, early_stop_patience)\u001b[0m\n\u001b[1;32m    249\u001b[0m state_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(partition_shuffles):\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m     \u001b[39m# Run model with barrier execution mode.\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     state_dict \u001b[39m=\u001b[39m mapPartitionsWithIndex(\n\u001b[1;32m    254\u001b[0m         rdd, \u001b[39mlambda\u001b[39;49;00m i, x: handle_model(\n\u001b[1;32m    255\u001b[0m             i,\n\u001b[1;32m    256\u001b[0m             x,\n\u001b[1;32m    257\u001b[0m             torch_obj\u001b[39m=\u001b[39;49mtorch_loaded,\n\u001b[1;32m    258\u001b[0m             master_url\u001b[39m=\u001b[39;49mmaster_url,\n\u001b[1;32m    259\u001b[0m             iters\u001b[39m=\u001b[39;49miters,\n\u001b[1;32m    260\u001b[0m             verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    261\u001b[0m             mini_batch\u001b[39m=\u001b[39;49mmini_batch,\n\u001b[1;32m    262\u001b[0m             validation_pct\u001b[39m=\u001b[39;49mvalidation_pct,\n\u001b[1;32m    263\u001b[0m             world_size\u001b[39m=\u001b[39;49mworld_size,\n\u001b[1;32m    264\u001b[0m             device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    265\u001b[0m             early_stop_patience\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(early_stop_patience\u001b[39m+\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    266\u001b[0m         )\n\u001b[1;32m    267\u001b[0m     )\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m partition_shuffles \u001b[39m-\u001b[39m i \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    270\u001b[0m         num_partitions \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mgetNumPartitions()\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(20, 0) finished unsuccessfully.\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/bitnami/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/bitnami/python/lib/python3.8/site-packages/sparktorch/distributed.py\", line 254, in <lambda>\n    rdd, lambda i, x: handle_model(\n  File \"/opt/bitnami/python/lib/python3.8/site-packages/sparktorch/distributed.py\", line 105, in handle_model\n    dist.init_process_group('gloo', rank=index + 1, world_size=world_size)\n  File \"/opt/bitnami/python/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 503, in init_process_group\n    _update_default_pg(_new_process_group_helper(\n  File \"/opt/bitnami/python/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 588, in _new_process_group_helper\n    pg = ProcessGroupGloo(\nRuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:769] connect [172.17.0.2]:24477: Connection refused\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2857)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 18:52:13 WARN TaskSetManager: Lost task 10.0 in stage 20.0 (TID 67) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                        (0 + 5) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 18:52:16 WARN PythonRunner: Incomplete task 9.0 in stage 20 (TID 69) interrupted: Attempting to kill Python Worker\n",
      "23/02/04 18:52:16 WARN PythonRunner: Incomplete task 4.0 in stage 20 (TID 65) interrupted: Attempting to kill Python Worker\n",
      "23/02/04 18:52:16 WARN PythonRunner: Incomplete task 1.0 in stage 20 (TID 68) interrupted: Attempting to kill Python Worker\n",
      "23/02/04 18:52:16 WARN PythonRunner: Incomplete task 8.0 in stage 20 (TID 61) interrupted: Attempting to kill Python Worker\n",
      "23/02/04 18:52:16 WARN PythonRunner: Incomplete task 4.0 in stage 20 (TID 65) interrupted: Attempting to kill Python Worker\n",
      "23/02/04 18:52:16 WARN TaskSetManager: Lost task 4.0 in stage 20.0 (TID 65) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n",
      "23/02/04 18:52:16 WARN PythonRunner: Incomplete task 9.0 in stage 20 (TID 69) interrupted: Attempting to kill Python Worker\n",
      "23/02/04 18:52:16 WARN TaskSetManager: Lost task 9.0 in stage 20.0 (TID 69) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n",
      "23/02/04 18:52:16 WARN PythonRunner: Incomplete task 7.0 in stage 20 (TID 66) interrupted: Attempting to kill Python Worker\n",
      "23/02/04 18:52:16 WARN PythonRunner: Incomplete task 8.0 in stage 20 (TID 61) interrupted: Attempting to kill Python Worker\n",
      "23/02/04 18:52:16 WARN TaskSetManager: Lost task 8.0 in stage 20.0 (TID 61) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                        (0 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 18:52:16 WARN PythonRunner: Incomplete task 7.0 in stage 20 (TID 66) interrupted: Attempting to kill Python Worker\n",
      "23/02/04 18:52:16 WARN TaskSetManager: Lost task 7.0 in stage 20.0 (TID 66) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n",
      "23/02/04 18:52:16 WARN PythonRunner: Incomplete task 1.0 in stage 20 (TID 68) interrupted: Attempting to kill Python Worker\n",
      "23/02/04 18:52:16 WARN TaskSetManager: Lost task 1.0 in stage 20.0 (TID 68) (c84f15796326 executor driver): TaskKilled (Task ResultTask(20, 0) from barrier stage ResultStage 20 (fit at /tmp/ipykernel_7295/2981807082.py:2) failed.)\n",
      "23/02/04 18:55:57 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-11ad814e-5f4f-4e7d-90aa-344e34f9504a. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-11ad814e-5f4f-4e7d-90aa-344e34f9504a\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:374)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:370)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:370)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:365)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2026)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 37720)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/bitnami/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/bitnami/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/opt/bitnami/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/bitnami/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "p = pyspark.ml.Pipeline(stages=[spark_model])\n",
    "model = p.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>               (0 + 12) / 12][Stage 36:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 18:23:06 WARN TaskSetManager: Stage 36 contains a task of very large size (16633 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|        prediction|      Weekly_Sales|\n",
      "+------------------+------------------+\n",
      "| 29245.37892680168|3440.6899999999996|\n",
      "|29337.512592778203|           3874.45|\n",
      "| 29684.89068370342|           3917.19|\n",
      "|27879.058874142167|           4061.89|\n",
      "| 29620.98361145258|           4136.05|\n",
      "| 27807.19763525009|           4150.96|\n",
      "| 28062.58364939928|           4159.25|\n",
      "| 28525.33491374254|           4204.65|\n",
      "| 29422.45038094282|           4205.62|\n",
      "| 29048.07909554958|           4238.56|\n",
      "|29009.868889975547|           4324.95|\n",
      "|28943.353430008887|           4377.74|\n",
      "|  27976.0600621891|           4417.17|\n",
      "|28853.091887905597|           4444.66|\n",
      "|28407.019831380843|           4444.92|\n",
      "| 28893.86643122196|           4484.47|\n",
      "| 29150.75958374977|           4544.62|\n",
      "|29044.636904764175|           4645.99|\n",
      "|28644.257256453035|           4660.39|\n",
      "| 28166.19233346939|           4756.69|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pred = model.transform(df_valid).select(\"prediction\", target)\n",
    "pred = pred.withColumn(\"prediction\", pred[\"prediction\"] * (max_target - min_target) + min_target)\n",
    "pred = pred.withColumn(target, pred[target] * (max_target - min_target) + min_target)\n",
    "pred[[\"prediction\", target]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/04 18:26:02 WARN TaskSetManager: Stage 37 contains a task of very large size (16633 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 12) / 12]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# get pred target max\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pred_pd \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39;49mtoPandas()\n\u001b[1;32m      3\u001b[0m pred_pd[\u001b[39m\"\u001b[39m\u001b[39mdiff\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pred_pd[\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m pred_pd[target]\n\u001b[1;32m      4\u001b[0m pred_pd[\u001b[39m\"\u001b[39m\u001b[39mdiff\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/pandas/conversion.py:205\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m pdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect(), columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m    206\u001b[0m column_counter \u001b[39m=\u001b[39m Counter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m    208\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema)\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[1;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get pred target max\n",
    "pred_pd = pred.toPandas()\n",
    "pred_pd[\"diff\"] = pred_pd[\"prediction\"] - pred_pd[target]\n",
    "pred_pd[\"diff\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
