{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/03 12:08:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Vu dep trai\").config(\"spark.executor.memory\",\"12g\").getOrCreate()\n",
    "# conf = pyspark.SparkConf().setMaster(\"spark://node-master:7077\")\\\n",
    "#         .setAppName(\"Vu dep trai\")\\\n",
    "#         .set(\"spark.executor.memory\",\"15g\")\n",
    "# # sc = SparkContext.getOrCreate(conf=conf)\n",
    "# # spark.stop()\n",
    "# sc = SparkContext(conf = conf)\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores_raw = spark.read.csv(\"data/ba-walmart/stores.csv\", header=True, inferSchema=True)\n",
    "df_feature_raw = spark.read.csv(\"data/ba-walmart/features.csv\", header=True, inferSchema=True)\n",
    "df_train_raw = spark.read.csv(\"data/ba-walmart/train.csv\", header=True, inferSchema=True)\n",
    "df_test_raw = spark.read.csv(\"data/ba-walmart/test.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_feature_raw.drop(\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\")\n",
    "df = df_train_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"], ).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "df_test = df_test_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"]).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "df = df.withColumn(\"CPI\", df[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df[\"Unemployment\"].cast(FloatType()))\n",
    "df_test = df_test.withColumn(\"CPI\", df_test[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df_test[\"Unemployment\"].cast(FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df_test = df_test.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df = df.withColumn(\"IsHoliday\", df[\"IsHoliday\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"IsHoliday\", df_test[\"IsHoliday\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.filter(df[\"Weekly_Sales\"] > 0)\n",
    "df_clean = df_clean.filter(df_clean[\"Weekly_Sales\"] < 450000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "types = df_clean.select(\"Type\").distinct().collect()\n",
    "types.sort()\n",
    "mapping = {t.Type: str(i) for i, t in enumerate(types)}\n",
    "df_clean = df_clean.replace(mapping, subset=[\"Type\"])\n",
    "df_test = df_test.replace(mapping, subset=[\"Type\"])\n",
    "df_clean = df_clean.withColumn(\"Type\", df_clean[\"Type\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"Type\", df_test[\"Type\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From EDA select important columns\n",
    "input_col = ['Store', 'IsHoliday', 'Type', 'Size', 'Week','Dept','Year']\n",
    "target = 'Weekly_Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = df_clean.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "RandomForestRegressor_167f86514a27 parameter maxDepth given invalid value 35.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m rf \u001b[39m=\u001b[39m RandomForestRegressor(featuresCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m, labelCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeekly_Sales\u001b[39m\u001b[39m\"\u001b[39m, maxDepth\u001b[39m=\u001b[39m\u001b[39m35\u001b[39m, numTrees\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, maxBins\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m)\n\u001b[1;32m      3\u001b[0m pipeline \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39mml\u001b[39m.\u001b[39mPipeline(stages\u001b[39m=\u001b[39m[assembler, rf])\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit(df_train)\n\u001b[1;32m      5\u001b[0m \u001b[39m# rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# pipeline = pyspark.ml.Pipeline(stages=[assembler, rf])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# paramGrid = pyspark.ml.tuning.ParamGridBuilder()\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m#                             numFolds=3)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# cvModel = crossval.fit(df_train)\u001b[39;00m\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    135\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    384\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/ml/wrapper.py:379\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39m    fitted Java model\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transfer_params_to_java()\n\u001b[1;32m    380\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj\u001b[39m.\u001b[39mfit(dataset\u001b[39m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/ml/wrapper.py:171\u001b[0m, in \u001b[0;36mJavaParams._transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams:\n\u001b[1;32m    170\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misSet(param):\n\u001b[0;32m--> 171\u001b[0m         pair \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_java_param_pair(param, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_paramMap[param])\n\u001b[1;32m    172\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj\u001b[39m.\u001b[39mset(pair)\n\u001b[1;32m    173\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhasDefault(param):\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/ml/wrapper.py:160\u001b[0m, in \u001b[0;36mJavaParams._make_java_param_pair\u001b[0;34m(self, param, value)\u001b[0m\n\u001b[1;32m    158\u001b[0m java_param \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj\u001b[39m.\u001b[39mgetParam(param\u001b[39m.\u001b[39mname)\n\u001b[1;32m    159\u001b[0m java_value \u001b[39m=\u001b[39m _py2java(sc, value)\n\u001b[0;32m--> 160\u001b[0m \u001b[39mreturn\u001b[39;00m java_param\u001b[39m.\u001b[39;49mw(java_value)\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: RandomForestRegressor_167f86514a27 parameter maxDepth given invalid value 35."
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=input_col, outputCol=\"features\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Weekly_Sales\", maxDepth=35, numTrees=20, seed=42, maxBins=100000)\n",
    "pipeline = pyspark.ml.Pipeline(stages=[assembler, rf])\n",
    "model = pipeline.fit(df_train)\n",
    "# rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target)\n",
    "# pipeline = pyspark.ml.Pipeline(stages=[assembler, rf])\n",
    "# paramGrid = pyspark.ml.tuning.ParamGridBuilder()\\\n",
    "#     .addGrid(rf.numTrees, [10])\\\n",
    "#     .addGrid(rf.maxDepth, [20])\\\n",
    "#     .addGrid(rf.maxBins, [100])\\\n",
    "#     .addGrid(rf.featureSubsetStrategy, [\"0.8\"])\\\n",
    "#     .addGrid(rf.subsamplingRate, [0.2])\\\n",
    "#     .build()\n",
    "# evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mse\")\n",
    "# crossval = pyspark.ml.tuning.CrossValidator(estimator=pipeline,\n",
    "#                             estimatorParamMaps=paramGrid,\n",
    "#                             evaluator=evaluator,\n",
    "#                             numFolds=3)\n",
    "# cvModel = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestModel = cvModel.bestModel\n",
    "# bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE:  7732.152219107972\n",
      "Valid MAE:  8610.979905817512\n"
     ]
    }
   ],
   "source": [
    "train_pred = model.transform(df_train)\n",
    "valid_pred = model.transform(df_valid)\n",
    "evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "train_mae = evaluator.evaluate(train_pred)\n",
    "valid_mae = evaluator.evaluate(valid_pred)\n",
    "print(\"Train MAE: \", train_mae)\n",
    "print(\"Valid MAE: \", valid_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
