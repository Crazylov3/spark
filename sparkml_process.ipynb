{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:40:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/05 16:40:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Vu dep trai\").config(\"spark.executor.memory\",\"12g\").getOrCreate()\n",
    "# conf = pyspark.SparkConf().setMaster(\"spark://node-master:7077\")\\\n",
    "#         .setAppName(\"Vu dep trai\")\\\n",
    "#         .set(\"spark.executor.memory\",\"15g\")\n",
    "# # sc = SparkContext.getOrCreate(conf=conf)\n",
    "# # spark.stop()\n",
    "# sc = SparkContext(conf = conf)\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores_raw = spark.read.csv(\"data/ba-walmart/stores.csv\", header=True, inferSchema=True)\n",
    "df_feature_raw = spark.read.csv(\"data/ba-walmart/features.csv\", header=True, inferSchema=True)\n",
    "df_train_raw = spark.read.csv(\"data/ba-walmart/train.csv\", header=True, inferSchema=True)\n",
    "df_test_raw = spark.read.csv(\"data/ba-walmart/test.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_feature_raw.drop(\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\")\n",
    "df = df_train_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"], ).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "df_test = df_test_raw.join(df_feature, how=\"left\", on=[\"Store\", \"Date\", \"IsHoliday\"]).join(df_stores_raw, how=\"left\", on=[\"Store\"])\n",
    "df = df.withColumn(\"CPI\", df[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df[\"Unemployment\"].cast(FloatType()))\n",
    "df_test = df_test.withColumn(\"CPI\", df_test[\"CPI\"].cast(FloatType())).withColumn(\"Unemployment\", df_test[\"Unemployment\"].cast(FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df_test = df_test.withColumn(\"Year\", year(\"Date\")).withColumn(\"Month\", month(\"Date\")).withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "df = df.withColumn(\"IsHoliday\", df[\"IsHoliday\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"IsHoliday\", df_test[\"IsHoliday\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean = df.filter(df[\"Weekly_Sales\"] > 0)\n",
    "# df_clean = df_clean.filter(df_clean[\"Weekly_Sales\"] < 450000)\n",
    "df_clean = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = df_clean.select(\"Type\").distinct().collect()\n",
    "types.sort()\n",
    "mapping = {t.Type: str(i) for i, t in enumerate(types)}\n",
    "df_clean = df_clean.replace(mapping, subset=[\"Type\"])\n",
    "df_test = df_test.replace(mapping, subset=[\"Type\"])\n",
    "df_clean = df_clean.withColumn(\"Type\", df_clean[\"Type\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"Type\", df_test[\"Type\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From EDA select important columns\n",
    "input_col = ['Store', 'IsHoliday', 'Type', 'Size', 'Week','Dept','Year']\n",
    "target = 'Weekly_Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = df_clean.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_col, outputCol=\"features_\")\n",
    "minmax = MinMaxScaler(inputCol=\"features_\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  14467.416166461382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "linear = LinearRegression(featuresCol=\"features\", labelCol=target, maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline = pyspark.ml.Pipeline(stages=[assembler, minmax, linear])\n",
    "model = pipeline.fit(df_train)\n",
    "pred = model.transform(df_valid)\n",
    "evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(pred)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7fdc96b903a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/pyspark/ml/wrapper.py\", line 53, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'FMRegressor' object has no attribute '_java_obj'\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  14366.208175575988\n"
     ]
    }
   ],
   "source": [
    "factor_reg = FMRegressor(featuresCol=\"features\", labelCol=target)\n",
    "pipeline = pyspark.ml.Pipeline(stages=[assembler, minmax, factor_reg])\n",
    "model = pipeline.fit(df_train)\n",
    "pred = model.transform(df_valid)\n",
    "evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(pred)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/bitnami/spark/jars/spark-core_2.12-3.3.1.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 478:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  8219.484677523957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gradient = GBTRegressor(featuresCol=\"features_\", labelCol=target)\n",
    "pipeline = pyspark.ml.Pipeline(stages=[assembler, gradient])\n",
    "model = pipeline.fit(df_train)\n",
    "pred = model.transform(df_valid)\n",
    "evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(pred)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:04 WARN DAGScheduler: Broadcasting large task binary with size 1532.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:05 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:06 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 524:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:07 WARN DAGScheduler: Broadcasting large task binary with size 1452.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:08 WARN DAGScheduler: Broadcasting large task binary with size 8.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 526:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:09 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:11 WARN DAGScheduler: Broadcasting large task binary with size 14.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 528:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:13 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:15 WARN DAGScheduler: Broadcasting large task binary with size 22.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 530:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:18 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 532:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:21 WARN DAGScheduler: Broadcasting large task binary with size 33.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 532:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:24 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 534:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:29 WARN DAGScheduler: Broadcasting large task binary with size 46.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 534:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:32 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 536:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:40 WARN DAGScheduler: Broadcasting large task binary with size 63.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 536:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:44 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 538:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:54 WARN DAGScheduler: Broadcasting large task binary with size 81.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 538:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 16:59:59 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 540:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:00:08 WARN DAGScheduler: Broadcasting large task binary with size 96.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 540:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:00:14 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 542:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:00:24 WARN DAGScheduler: Broadcasting large task binary with size 114.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 542:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:00:30 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 544:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:00:41 WARN DAGScheduler: Broadcasting large task binary with size 138.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 544:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:00:48 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 546:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:01:00 WARN DAGScheduler: Broadcasting large task binary with size 155.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 546:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:01:06 WARN DAGScheduler: Broadcasting large task binary with size 10.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 548:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:01:20 WARN DAGScheduler: Broadcasting large task binary with size 171.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 548:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:01:27 WARN DAGScheduler: Broadcasting large task binary with size 8.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 550:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:01:41 WARN DAGScheduler: Broadcasting large task binary with size 184.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 550:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:01:47 WARN DAGScheduler: Broadcasting large task binary with size 7.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 552:>                                                        (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:02:01 WARN DAGScheduler: Broadcasting large task binary with size 195.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 552:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:02:08 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:02:10 WARN DAGScheduler: Broadcasting large task binary with size 19.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:02:11 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 17:02:13 WARN DAGScheduler: Broadcasting large task binary with size 10.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 562:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  3483.14591347027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor(featuresCol=\"features_\", labelCol=target, numTrees=20, maxDepth=25, maxBins=100000)\n",
    "pipeline = pyspark.ml.Pipeline(stages=[assembler, random_forest])\n",
    "model = pipeline.fit(df_train)\n",
    "pred = model.transform(df_valid)\n",
    "evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(pred)\n",
    "print(\"MAE: \", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembler = VectorAssembler(inputCols=input_col, outputCol=\"features\")\n",
    "# rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Weekly_Sales\", maxDepth=30, numTrees=20, seed=42, maxBins=100000)\n",
    "# pipeline = pyspark.ml.Pipeline(stages=[assembler, rf])\n",
    "# model = pipeline.fit(df_train)\n",
    "# rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target)\n",
    "# pipeline = pyspark.ml.Pipeline(stages=[assembler, rf])\n",
    "# paramGrid = pyspark.ml.tuning.ParamGridBuilder()\\\n",
    "#     .addGrid(rf.numTrees, [10])\\\n",
    "#     .addGrid(rf.maxDepth, [20])\\\n",
    "#     .addGrid(rf.maxBins, [100])\\\n",
    "#     .addGrid(rf.featureSubsetStrategy, [\"0.8\"])\\\n",
    "#     .addGrid(rf.subsamplingRate, [0.2])\\\n",
    "#     .build()\n",
    "# evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mse\")\n",
    "# crossval = pyspark.ml.tuning.CrossValidator(estimator=pipeline,\n",
    "#                             estimatorParamMaps=paramGrid,\n",
    "#                             evaluator=evaluator,\n",
    "#                             numFolds=3)\n",
    "# cvModel = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestModel = cvModel.bestModel\n",
    "# bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pred = model.transform(df_train)\n",
    "# valid_pred = model.transform(df_valid)\n",
    "# evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "# train_mae = evaluator.evaluate(train_pred)\n",
    "# valid_mae = evaluator.evaluate(valid_pred)\n",
    "# print(\"Train MAE: \", train_mae)\n",
    "# print(\"Valid MAE: \", valid_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
